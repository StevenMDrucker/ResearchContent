[
    {
        "caption": "Task Level Learning",
        "img": "researchImages/robotlearn.gif",
        "bibEntry": "<div class=\"csl-entry\">Aboaf, E. W., Drucker, S. M., and Atkeson, C. G. (1989). Task-level robot learning: Juggling a tennis ball more accurately. In <i>Robotics and Automation, 1989. Proceedings., 1989 IEEE International Conference on</i> (pp. 1290â1295). IEEE.</div>",
        "tags": {
            "collaborators": [
                "Aboaf",
                "Atkeson"
            ],
            "subject": [
                "Robotics",
                "Learning",
                "Thesis"
            ],
            "year": [
                "1989"
            ],
            "publication": [
                "IEEE",
                "MastersThesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/tasklevel.pdf",
        "abstract": "We report on a preliminary investigation of task-level learning, an approach to learning from practice. We have a programmaed a robot to juggle a single ball in three dimensions by batting it upwards with a large paddle. The robot uses a real-time binary vision system to track the ball and measure its performance. Task-level learning consists of building a model of performance errors at the task level during practice, and using that model to refine task-level commands. A polynomial surface was fit to the errors in where the ball went after each hit, and this task model is used to refine how the ball is hit. This application of task-level learning dramatically increased the number of consecutive hits the robot could execute before the ball was hit out of range of the paddle.",
        "primary": "Robotics"
    },
    {
        "caption": "Interactive Digital Photomontage",
        "img": "researchImages/photomontage.jpg",
        "bibEntry": "<div class=\"csl-entry\">Agarwala, A., Dontcheva, M., Agrawala, M., Drucker, S., Colburn, A., Curless, B., â¦ Cohen, M. (2004). Interactive digital photomontage. In <i>ACM Transactions on Graphics (TOG)</i> (Vol. 23, pp. 294â302). ACM.</div>",
        "tags": {
            "collaborators": [
                "Agarwala",
                "Dontcheva",
                "Agrawala",
                "Colburn",
                "Curless",
                "Salesin",
                "Cohen"
            ],
            "subject": [
                "Graphics",
                "Photos"
            ],
            "year": [
                "2004"
            ],
            "publication": [
                "SIGGRAPH"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/photomontage.pdf",
        "video": "http://grail.cs.washington.edu/projects/photomontage/video.avi",
        "abstract": "We describe an interactive, computer-assisted framework for combining parts of a set of photographs into a single composite picture, a process we call 'digital photomontage.' Our framework makes use of two techniques primarily: graph-cut optimization, to choose good seams within the constituent images so that they can be combined as seamlessly as possible; and gradient-domain fusion, a process based on Poisson equations, to further reduce any remaining visible artifacts in the composite. Also central to the framework is a suite of interactive tools that allow the user to specify a variety of high-level image objectives, either globally across the image, or locally through a painting-style interface. Image objectives are applied independently at each pixel location and generally involve a function of the pixel values (such as 'maximum contrast') drawn from that same location in the set of source images. Typically, a user applies a series of image objectives iteratively in order to create a finished composite. The power of this framework lies in its generality; we show how it can be used for a wide variety of applications, including 'selective composites' (for instance, group photos in which everyone looks their best), relighting, extended depth of field, panoramic stitching, clean-plate production, stroboscopic visualization of movement, and time-lapse mosaics.",
        "primary": "Photos"
    },
    {
        "caption": "Geospatial Stream",
        "img": "researchImages/geospatial.png",
        "bibEntry": "<div class=\"csl-entry\">Ali, M., Chandramouli, B., Fay, J., Wong, C., Drucker, S., and Raman, B. S. (2011). Online visualization of geospatial stream data using the WorldWide telescope. In <i>Proceedings of the International Conference on Very Large Data Bases (VLDB)</i>.</div>",
        "tags": {
            "collaborators": [
                "Ali",
                "Chandramouli",
                "Fay",
                "Wong",
                "Raman"
            ],
            "subject": [
                "Visualization"
            ],
            "year": [
                "2011"
            ],
            "publication": [
                "VLDB"
            ]
        },
        "primary": "Visualization"
    },
    {
        "caption": "Modeltracker",
        "img": "researchImages/modeltracker.png",
        "bibEntry": "<div class=\"csl-entry\">Amershi, S., Chickering, M., Drucker, S. M., Lee, B., Simard, P., and Suh, J. (2015). Modeltracker: Redesigning performance analysis tools for machine learning. In <i>Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</i> (pp. 337â346). ACM.</div>",
        "tags": {
            "collaborators": [
                "Amershi",
                "Chickering",
                "Lee",
                "Simard",
                "Suh"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Touch",
                "Sequences"
            ],
            "year": [
                "2015"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/pn2048-amershi-fixed.pdf",
        "video": "http://research.microsoft.com/en-us/um/people/sdrucker/video/squeries_v1.0.mp4",
        "abstract": "Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with  ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker???s debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.",
        "primary": "Machine Learning"
    },
    {
        "caption": "Informal Decisions",
        "img": "researchImages/informaldecisions.png",
        "bibEntry": "<div class=\"csl-entry\">AndrÃ©, P., Drucker, S., and schraefel,  m. c. (2007). <i>Informal Online Decision Making: Current Practices and Support System Design</i>. Technical Report.</div>",
        "tags": {
            "collaborators": [
                "Andre",
                "schraefel"
            ],
            "subject": [
                "UI",
                "Information"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "Internal Report"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/vidido.pdf",
        "abstract": "Existing group decision support systems are too complex to support lightweight, informal decision making made popular by the amount of information available on the Web. From an examination of related work, an online survey and a formative study to examine how people currently use the Web for decision support, we present a set of design recommendations towards the development of an informal Web decision support tool.",
        "primary": "UI-Information"
    },
    {
        "caption": "Bones",
        "img": "researchImages/bones.PNG",
        "bibEntry": "<div class=\"csl-entry\">Barik, T., DeLine, R., Drucker, S., and Fisher, D. (2016). The bones of the system: a case study of logging and telemetry at Microsoft. In <i>Proceedings of the 38th International Conference on Software Engineering Companion</i> (pp. 92â101). ACM.</div>",
        "tags": {
            "collaborators": [
                "Barik",
                "DeLine",
                "Fisher"
            ],
            "subject": [
                "Information"
            ],
            "year": [
                "2016"
            ],
            "publication": [
                "ICSE"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/PID4092213.pdf",
        "abstract": "Large software organizations are transitioning to event data platforms as they culturally shift to better support datadriven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in datadriven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.",
        "primary": "UI-Information"
    },
    {
        "caption": "Stat!",
        "img": "researchImages/stat.png",
        "bibEntry": "<div class=\"csl-entry\">Barnett, M., Chandramouli, B., DeLine, R., Drucker, S., Fisher, D., Goldstein, J., â¦ Platt, J. (2013). Stat!: an interactive analytics environment for big data. In <i>Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</i> (pp. 1013â1016). ACM.</div>",
        "tags": {
            "collaborators": [
                "Barnett",
                "Chandramouli",
                "DeLine",
                "Fisher",
                "Goldstein",
                "Morrison",
                "Platt"
            ],
            "subject": [
                "Information"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "Sigmod"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/stat-sigmod2013-demo.pdf",
        "video": "http://research.microsoft.com/en-us/people/sdrucker/ResearchContent/papers/",
        "abstract": "Exploratory analysis on big data requires us to rethink data management across the entire stack ??? from the underlying data processing techniques to the user experience. We demonstrate Stat! ??? a visualization and analytics environment that allows users to rapidly experiment with exploratory queries over big data. Data scientists can use Stat! to quickly refine to the correct query, while getting immediate feedback after processing a fraction of the data. Stat! can work with multiple processing engines in the backend; in this demo, we use Stat! with the Microsoft StreamInsight streaming engine. StreamInsight is used to generate incremental early results to queries and refine these results as more data is processed. Stat! allows data scientists to explore data, dynamically compose multiple queries to generate streams of partial results, and display partial results in both textual and visual form",
        "primary": "UI-Information"
    },
    {
        "caption": "iClusterTheory",
        "img": "researchImages/iclustering2.png",
        "bibEntry": "<div class=\"csl-entry\">Basu, S., Fisher, D., Drucker, S. M., and Lu, H. (2010). Assisting Users with Clustering Tasks by Combining Metric Learning and Classification. In <i>AAAI</i>.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Basu",
                "Lu"
            ],
            "subject": [
                "UI",
                "Machine Learning"
            ],
            "year": [
                "2010"
            ],
            "publication": [
                "AAAI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/iclustering-aaai-2010.pdf",
        "abstract": "Interactive clustering refers to situations in which a human labeler is willing to assist a learning algorithm in automatically clustering items. We present a related but somewhat different task, assisted clustering, in which a user creates explicit groups of items from a large set and wants suggestions on what items to add to each group. While the traditional approach to interactive clustering has been to use metric learning to induce a distance metric, our situation seems equally amenable to classification. Using clusterings of documents from human subjects, we found that one or the other method proved to be superior for a given cluster, but not uniformly so. We thus developed a hybrid mechanism for combining the metric learner and the classifier. We present results from a large number of trials based on human clusterings, in which we show that our combination scheme matches and often exceeds the performance of a method which exclusively uses either type of learner.",
        "primary": "Machine Learning"
    },
    {
        "caption": "DesignReflections",
        "img": "researchImages/designreflections.png",
        "bibEntry": "<div class=\"csl-entry\">Bigelow, A., Drucker, S., Fisher, D., and Meyer, M. (2014). Reflections on how designers design with data. In <i>Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces</i> (pp. 17â24). ACM.</div>",
        "tags": {
            "collaborators": [
                "Bigelow",
                "Fisher",
                "Meyer"
            ],
            "subject": [
                "Visualization",
                "Information",
                "Design"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "AVI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/design-vis.pdf",
        "video": "http://research.microsoft.com/apps/pubs/default.aspx?id=208568",
        "abstract": "In recent years many popular data visualizations have emerged that are created largely by designers whose main area of expertise is not computer science. Designers generate these visualizations using a handful of design tools and environments. To better inform the development of tools intended for designers working with data, we set out to understand designers' challenges and perspectives. We interviewed professional designers, conducted observations of designers working with data in the lab, and observed designers working with data in team settings in the wild. A set of patterns emerged from these observations from which we extract a number of themes that provide a new perspective on design considerations for visualization tool creators, as well as on known engineering problems.",
        "primary": "Visualization"
    },
    {
        "caption": "Hanpuku",
        "img": "researchImages/hanpuku.png",
        "bibEntry": "<div class=\"csl-entry\">Bigelow, A., Drucker, S., Fisher, D., and Meyer, M. (2017). Iterating between tools to create and edit visualizations. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>23</i>(1), 481â490.</div>",
        "tags": {
            "collaborators": [
                "Bigelow",
                "Fisher",
                "Meyer"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Design"
            ],
            "year": [
                "2016"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/hanpuku.pdf",
        "abstract": "A common work?ow for visualization designers begins with a generative tool, like D3 or Processing, to create the initial visualization;andproceedstoadrawingtool,likeAdobeIllustratororInkscape,foreditingandcleaning. Unfortunately,thisistypically a one-way process: once a visualization is exported from the generative tool into a drawing tool, it is dif?cult to make further, datadriven changes. In this paper, we propose a bridge model to allow designers to bring their work back from the drawing tool to re-edit in the generative tool. Our key insight is to recast this iteration challenge as a merge problem - similar to when two people are editing a document and changes between them need to reconciled. We also present a speci?c instantiation of this model, a tool called Hanpuku, which bridges between D3 scripts and Illustrator. We show several examples of visualizations that are iteratively created using Hanpuku in order to illustrate the ?exibility of the approach. We further describe several hypothetical tools that bridge between other visualization tools to emphasize the generality of the model.",
        "primary": "Visualization"
    },
    {
        "caption": "Feature Insight",
        "img": "researchImages/featureinsight.png",
        "bibEntry": "<div class=\"csl-entry\">Brooks, M., Amershi, S., Lee, B., Drucker, S. M., Kapoor, A., and Simard, P. (2015). FeatureInsight: Visual support for error-driven feature ideation in text classification. In <i>Visual Analytics Science and Technology (VAST), 2015 IEEE Conference on</i> (pp. 105â112). IEEE.</div>",
        "tags": {
            "collaborators": [
                "Brooks",
                "Amershi",
                "Lee",
                "Kapoor",
                "Simard"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Machine Learning"
            ],
            "year": [
                "2015"
            ],
            "publication": [
                ""
            ]
        },
        "abstract": "Machine learning requires an effective combination of data,  features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.",
        "primary": "UI-Information"
    },
    {
        "caption": "Terrain Registration",
        "img": "researchImages/registration.png",
        "bibEntry": "<div class=\"csl-entry\">Chen, B., Ramos, G., Ofek, E., Cohen, M., Drucker, S., and NistÃ©r, D. (2008). <i>Interactive techniques for registering images to digital terrain and building models</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Chen",
                "Ramos",
                "Ofek",
                "Cohen"
            ],
            "subject": [
                "Graphics"
            ],
            "year": [
                "2008"
            ],
            "publication": [
                "Internal Report"
            ]
        },
        "abstract": "We investigate two interactive techniques for registering an image to 3D digital terrain and building models. Registering an image enables a variety of applications, including slideshows with context, automatic annotation, and photo enhancement. To perform the registration, we investigate two modes of interaction. In the overlay interface, an image is displayed over a 3D view and a user manually aligns 3D points to points in the image. In the split interface, the image and the 3D view are displayed side-by-side and the user indicates matching points across the two views. Our user study suggests that the overlay interface is more engaging than split, but is less accurate in registration. We then show several applications that make use of the registration data.",
        "primary": "Graphics"
    },
    {
        "caption": "Flatland",
        "img": "researchImages/flatland.jpg",
        "bibEntry": "<div class=\"csl-entry\">Chesley, H., Drucker, S. M., Gupta, A., Kimberly, G., and White, S. (2001). <i>Flatland, Rapid prototyping of distributed internet applications</i> (No. MSR-TR-2001-73). Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Chesley",
                "Gupta",
                "Kimberly",
                "White"
            ],
            "subject": [
                "UI",
                "Education",
                "Social"
            ],
            "year": [
                "2001"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/flatland.pdf",
        "abstract": "Computer intra- and internets are widely used for client-server application such as web browsers. With the exception of e-mail, however, the same networks are seldom used for distributed, client-client or client-server-client applications. Such applications are difficult to develop and debug, and require a supporting infrastructure that is not readily available from existing systems. Flatland is a rapid prototyping environment that provides the underlying infrastructure and makes it easy to create and debug distributed internet application prototypes. In addition to the infrastructure needed for a distributed application, Flatland includes safe implementations of the most common sources of distributed application bugs, asynchronous operation and updating. Flatland also supports streaming audio-video and down-level clients.",
        "primary": "Social"
    },
    {
        "caption": "Demowiz",
        "img": "researchImages/demowiz.png",
        "bibEntry": "<div class=\"csl-entry\">Chi, P.-Y., Lee, B., and Drucker, S. M. (2014). DemoWiz: re-performing software demonstrations for a live presentation. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 1581â1590). ACM.</div>",
        "tags": {
            "collaborators": [
                "Chi",
                "Lee"
            ],
            "subject": [
                "UI",
                "Presentation"
            ],
            "year": [
                "2014"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/demowiz.pdf",
        "abstract": "Showing a live software demonstration during a talk can be engaging, but it is often not easy: presenters may struggle with (or worry about) unexpected software crashes and  encounter issues such as mismatched screen resolutions or faulty network connectivity.  Furthermore, it can be difficult to recall the steps to show while talking and operating the  system all at the same time. An alternative is to present with pre-recorded screencast videos. It is, however, challenging to precisely match the narration to the video when using existing video players. We introduce DemoWiz, a video presentation system that provides an increased awareness of upcoming actions through glanceable visualizations. DemoWiz supports better control of timing by overlaying visual cues and enabling lightweight editing. A user study shows that our design significantly improves the presenters??? perceived ease of narration and timing compared to a system without visualizations that was similar to a standard playback control. Furthermore, nine (out of ten) participants preferred DemoWiz over the standard playback control with the last expressing no preference.",
        "primary": "Presentation"
    },
    {
        "caption": "Avatar Eye Gaze",
        "img": "researchImages/eyegaze.jpg",
        "bibEntry": "<div class=\"csl-entry\">Colburn, A., Cohen, M. F., and Drucker, S. (2000). The role of eye gaze in avatar mediated conversational interfaces. <i>Sketches and Applications, Siggraphâ00</i>.</div>",
        "tags": {
            "collaborators": [
                "Colburn",
                "Cohen"
            ],
            "subject": [
                "Graphics",
                "Animation"
            ],
            "year": [
                "2000"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/tr-2000-81.pdf",
        "abstract": "As we begin to create synthetic characters (avatars) for computer users, it is important to pay attention to both the look and the behavior of the avatar's eyes. In this paper we present behavior models of eye gaze patterns in the context of real-time verbal communication. We apply these eye gaze models to simulate eye movements in a computer-generated avatar in a number of task settings. We also report the results of an experiment that we conducted to assess whether our eye gaze model induces changes in the eye gaze behavior of an individual who is conversing with an avatar.",
        "primary": "Graphics"
    },
    {
        "caption": "Graphic enhancement for conference calls",
        "img": "researchImages/conferencecalls.jpg",
        "bibEntry": "<div class=\"csl-entry\">Colburn, R. A., Cohen, M. F., Drucker, S. M., LeeTiernan, S., and Gupta, A. (2001). Graphical enhancements for voice only conference calls. <i>Microsoft Corporation, Redmond, WA, Technical Report MSR-TR-2001-95</i>.</div>",
        "tags": {
            "collaborators": [
                "Colburn",
                "Cohen",
                "Counts",
                "Gupta"
            ],
            "subject": [
                "Graphics",
                "Social"
            ],
            "year": [
                "2000"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/tr-2001-95.pdf",
        "abstract": "We present two very low bandwidth graphically enhanced interfaces for small group voice communications. One interface presents static images of the participants that highlight when one is speaking. The other interface utilizes three-dimensional avatars that can be quickly created. Eleven groups of 4 or 5 people were presented with each enhanced interface as well as conducting a live conversation and a voice only conversation. Experiments show that both graphically enhanced interfaces improve the understandability of conversations, particular with respect to impressions that others in the group could express themselves more easily, knowing who is talking, and when to speak. Little difference was found between the two graphical interfaces. Analysis of voice tracks also revealed differences between interfaces in the length and number of medium duration silences.",
        "primary": "Social"
    },
    {
        "caption": "Code Thumbnails",
        "img": "researchImages/codethumb.jpg",
        "bibEntry": "<div class=\"csl-entry\">DeLine, R., Czerwinski, M., Meyers, B., Venolia, G., Drucker, S., and Robertson, G. (2006). Code thumbnails: Using spatial memory to navigate source code. In <i>Visual Languages and Human-Centric Computing (VL/HCCâ06)</i> (pp. 11â18). IEEE.</div>",
        "tags": {
            "collaborators": [
                "DeLine",
                "Czerwinski",
                "Meyers",
                "Venolia",
                "Robertson"
            ],
            "subject": [
                "Visualization",
                "Programming"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "VLL/HCC"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/vlhcc06-final.pdf",
        "abstract": "Modern development environments provide many features for navigating source code, yet recent studies show the developers still spend a tremendous amount of time just navigating. Since existing navigation features rely heavily on memorizing symbol names, we present a new design, called Code Thumbnails, intended to allow a developer to navigate source code by forming a spa-tial memory of it. To aid intra-file navigation, we add a thumbnail image of the file to the scrollbar, which makes any part of the file one click away. To aid inter-file navigation, we provide a desktop of file thumbnail images, which make any part of any file one click away. We did a formative evaluation of the design with eleven experienced developers and present the results.",
        "primary": "Visualization"
    },
    {
        "caption": "V4V",
        "img": "researchImages/v4v.jpg",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Drucker, S. M., and Cohen, M. F. (2005). v4v: a View for the Viewer. In <i>Proceedings of the 2005 conference on Designing for User eXperience</i> (p. 19). AIGA: American Institute of Graphic Arts.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Cohen"
            ],
            "subject": [
                "Animation",
                "UI",
                "Presentation"
            ],
            "year": [
                "2005"
            ],
            "publication": [
                "DUX"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/v4v.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/v4v.avi",
        "abstract": "We present a View for the Viewer (v4v), a slide viewer that focuses on the needs of the viewer of a presentation instead of the presenter. Our design centers on representing the deck of slides as a stack embedded in a 3-D world. With only single button clicks, the viewer can quickly and easily navigate the deck of slides. We provide four types of annotation techniques and have designed a synchronization mechanism that makes it easy for the viewer to move in and out of sync with the presenter. We also supply alarms as a method for viewer notification. We evaluate our approach with a preliminary user study resulting in positive feedback about our design plus suggestions for improvements and extensions.",
        "primary": "Presentation"
    },
    {
        "caption": "Relations, Templates",
        "img": "researchImages/relations.png",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Drucker, S. M., Salesin, D., and Cohen, M. F. (2007). Relations, cards, and search templates: user-guided web data integration and layout. In <i>Proceedings of the 20th annual ACM symposium on User interface software and technology</i> (pp. 61â70). ACM.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Cohen",
                "Salesin"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Search"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/DontchevaUist07.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/DontchevaUist07.mov",
        "abstract": "in collecting and organizing Web content. First, we demonstrate an interface for creating associations between websites, which facilitate the automatic retrieval of related content. Second, we present an authoring interface that allows users to quickly merge content from many different websites into a uniform and personalized representation, which we call a card. Finally, we introduce a novel search paradigm that leverages the relationships in a card to direct search queries to extract relevant content from multipleWeb sources and fill a new series of cards instead of just returning a list of webpage URLs. Preliminary feedback from users is positive and validates our design",
        "primary": "UI-Information"
    },
    {
        "caption": "Web Summary Templates",
        "img": "researchImages/contentextraction.png",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Drucker, S. M., Salesin, D., and Cohen, M. F. (2010). From Web Summaries to search templates. <i>No Code Required: Giving Users Tools to Transform the Web</i>, 235.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Salesin",
                "Cohen"
            ],
            "subject": [
                "Information",
                "UI"
            ],
            "year": [
                "2010"
            ],
            "publication": [
                "Book"
            ]
        },
        "primary": "UI-Information"
    },
    {
        "caption": "Summarizing Web Sessions",
        "img": "researchImages/webpage_photo.gif",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Drucker, S. M., Wade, G., Salesin, D., and Cohen, M. F. (2006a). Collecting and organizing web content. In <i>Personal Information Management-Special Interest Group for Information Retrieval Workshop</i> (pp. 44â47).</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Salesin",
                "Wade",
                "Cohen"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Web",
                "Search"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/uistPaperSummarizing.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/uistSummariesfinalCut.mov",
        "abstract": "We describe a system, implemented as a browser extension, that enables users to quickly and easily collect, view, and share personal Web content. Our system employs a novel interaction model, which allows a user to specify webpage extraction patterns by interactively selecting webpage elements and applying these patterns to automatically collect similar content. Further, we present a technique for creating visual summaries of the collected information by combining user labeling with predefined layout templates. These summaries are interactive in nature: depending on the behaviors encoded in their templates, they may respond to mouse events, in addition to providing a visual summary. Finally, the summaries can be saved or sent to others to continue the research at another place or time. Informal evaluation shows that our approach works well for popular websites, and that users can quickly learn this interaction model for collecting content from the Web.",
        "primary": "UI-Information"
    },
    {
        "caption": "Collecting Web Content",
        "img": "researchImages/collecting.png",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Drucker, S. M., Wade, G., Salesin, D., and Cohen, M. F. (2006b). Summarizing personal web browsing sessions. In <i>Proceedings of the 19th annual ACM symposium on User interface software and technology</i> (pp. 115â124). ACM.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Wade",
                "Salesin",
                "Cohen"
            ],
            "subject": [
                "UI",
                "Information"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "Book"
            ]
        },
        "abstract": "Our work focuses on lowering user overhead for collecting and organizing Web content through the use of automation and visualization. We discuss a framework that enables users to semi-automatically collect, view, and share personal Web content. Our approach allows a user to interactively select webpage elements of interest and automatically collect similar content. Further, we describe a technique for creating visual summaries of the collected information by combining user labeling with predefined layout templates. These summaries are interactive in nature and provide a variety of visual representations for the collected content. Finally, the summaries can be saved or sent to other users to continue the research at another place or time",
        "primary": "UI-Information"
    },
    {
        "caption": "Content Extraction",
        "img": "researchImages/contentextraction.png",
        "bibEntry": "<div class=\"csl-entry\">Dontcheva, M., Lin, S., Drucker, S. M., Salesin, D., and Cohen, M. F. (2008). Experiences with content extraction from the web. In <i>Proc SUI</i>.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Lin",
                "Salesin",
                "Cohen"
            ],
            "subject": [
                "Information",
                "UI"
            ],
            "year": [
                "2008"
            ],
            "publication": [
                "SUI"
            ]
        },
        "primary": "UI-Information"
    },
    {
        "caption": "Spectator (concept)",
        "img": "researchImages/spectator_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S., He, L., Cohen, M., Wong, C., and Gupta, A. (2002). <i>Spectator games: A new entertainment modality of networked multiplayer games</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "He",
                "Cohen",
                "Gupta",
                "Wong",
                "Roseway",
                "De Mar"
            ],
            "subject": [
                "Graphics",
                "UI",
                "Games"
            ],
            "year": [
                "2003"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/spectator.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/spectator.asf",
        "abstract": "Networked multiplayer games are becoming tremendously popular. At any given moment on the Microsoft Game Zone (http://zone.msn.com), there are thousands of people playing Asheron's Call or Age of Empires. Traditional board and card games are also increasingly being played online and will continue to gain in popularity. While networked games are certainly fun for active players, there is potentially a much larger audience: spectators. In most traditional games, such as football, the number of spectators far exceeds the number of players. The key idea presented in this paper is to tap this potential by making online games engaging and entertaining to non-players watching these games. <br> The experience for spectators can be made much richer by employing techniques often used in sports broadcasting, such as a commentator providing analysis and background stories, slow motion and instance replay. For 3D games, cinematic camera movements and shot cuts be much more visually interesting than the first-person views often provided to the players. There is the potential to significantly increase the 'eyeballs' on sites such as Microsoft Game Zone. Spectators can be more easily targeted for advertising. Finally, supporting the spectator experience will help drive sales of the games themselves as casual viewers take the next step to become players. Watching others play networked games has the potential to become a vital component to an overall entertainment/media strategy. The authors of this document have already developed significant technologies needed to support the online game spectator. We propose that new resources be devoted now to carry these technologies into practice.",
        "primary": "Camera"
    },
    {
        "caption": "Phototriage",
        "img": "researchImages/phototriage.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S., Wong, C., Roseway, A., Glenner, S., and De Mar, S. (2003). <i>Photo-triage: Rapidly annotating your digital photographs</i> (No. MSR-TR-2003-99). Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Wong",
                "Roseway",
                "Glenner",
                "De Mar"
            ],
            "subject": [
                "UI",
                "Photos"
            ],
            "year": [
                "2003"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/phototriage.pdf",
        "abstract": "The Photo-triage application is meant to be an essential part of the digital photography lifestyle. It can fit as a component wherever photo management is done (shell, picture-it, media-center, etc.). The central idea is to facilitate rapid, convenient categorization of one's personal photos into at least the following categories: hidden/private, majority, highlights, best and/or representative. See figure 1. This application is meant to fill an empty niche in the usage of digital photos: that is, there's no easy way add metadata to photos to mark them for printing, for sharing, or for slideshows without creating separate versions of the photos and copying into separate folders. We propose a sorting metaphor that will add implicit metadata when one first goes through the photos.",
        "primary": "Photos"
    },
    {
        "caption": "Texture from Touch",
        "img": "researchImages/natcomp.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M. (1988). Texture from Touch. In W. Richards (Ed.), <i>Natural Computation</i>. MIT Press.</div>",
        "tags": {
            "collaborators": [
                "Individual"
            ],
            "subject": [
                "Robotics",
                "Touch"
            ],
            "year": [
                "1988"
            ],
            "publication": [
                "Book"
            ]
        },
        "abstract": "Not available",
        "primary": "Robotics"
    },
    {
        "caption": "MOOs to Multi-user apps",
        "img": "researchImages/uistmoos.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M. (1999). <i>Moving from MOOs to Multi-User Applications</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Individual"
            ],
            "subject": [
                "Graphics",
                "Social"
            ],
            "year": [
                "1999"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/moving.pdf",
        "abstract": "This paper provides a brief description of the work we have done on the V-Worlds project, a system that facilitates the creation of multi-user applications and environments. We have taken concepts originally found in object oriented Multi-User Dungeons (MOOs) and extended them to deal with more general multi-user and in particular multi-media applications. We present reasons behind the architectural decisions of the platform and show that it has been used successfully for a wide range of examples.",
        "primary": "Social"
    },
    {
        "caption": "Spectator (MechWarrior)",
        "img": "researchImages/spectator3.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and De Mar, S. (2002). <i>Spectator implementation in Mechwarrior</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "He",
                "Cohen",
                "Gupta",
                "Wong",
                "De Mar"
            ],
            "subject": [
                "Graphics",
                "UI",
                "Games",
                "Thesis"
            ],
            "year": [
                "2003"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/spectator.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/spectator3.wmv",
        "abstract": "Networked multiplayer games are becoming tremendously popular. At any given moment on the Microsoft Game Zone (http://zone.msn.com), there are thousands of people playing Asheron's Call or Age of Empires. Traditional board and card games are also increasingly being played online and will continue to gain in popularity. While networked games are certainly fun for active players, there is potentially a much larger audience: spectators. In most traditional games, such as football, the number of spectators far exceeds the number of players. The key idea presented in this paper is to tap this potential by making online games engaging and entertaining to non-players watching these games. <br> The experience for spectators can be made much richer by employing techniques often used in sports broadcasting, such as a commentator providing analysis and background stories, slow motion and instance replay. For 3D games, cinematic camera movements and shot cuts be much more visually interesting than the first-person views often provided to the players. There is the potential to significantly increase the 'eyeballs' on sites such as Microsoft Game Zone. Spectators can be more easily targeted for advertising. Finally, supporting the spectator experience will help drive sales of the games themselves as casual viewers take the next step to become players. Watching others play networked games has the potential to become a vital component to an overall entertainment/media strategy. The authors of this document have already developed significant technologies needed to support the online game spectator. We propose that new resources be devoted now to carry these technologies into practice.",
        "primary": "Camera"
    },
    {
        "caption": "iCluster",
        "img": "researchImages/iclustering1.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Fisher, D., and Basu, S. (2011). Helping users sort faster with adaptive machine learning recommendations. In <i>IFIP Conference on Human-Computer Interaction</i> (pp. 187â203). Springer Berlin Heidelberg.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Basu"
            ],
            "subject": [
                "UI",
                "Visualization",
                "Machine Learning"
            ],
            "year": [
                "2011"
            ],
            "publication": [
                "Interact"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/icluster_nonanonymous_sdrucker_cameraready.pdf",
        "video": "http://www.youtube.com/watch?feature=player_embedded&v=3BmO3TILucQ",
        "abstract": "Sorting and clustering large numbers of documents can be an overwhelming task: manual solutions tend to be slow, while machine learning systems often present results that don???t align well with users' intents. We created and evaluated a system for helping users sort large numbers of documents into clusters. iCluster has the capability to recommend new items for existing clusters and appropriate clusters for items. The recommendations are based on a learning model that adapts over time ??? as the user adds more items to a cluster, the system???s model improves and the recommendations become more relevant. Thirty-two subjects used iCluster to sort hundreds of data items both with and without recommendations; we found that recommendations allow users to sort items more rapidly. A pool of 161 raters then assessed the quality of the resulting clusters, finding that clusters generated with recommendations were of statistically indistinguishable quality. Both the manual and assisted methods were substantially better than a fully automatic method.",
        "primary": "Visualization"
    },
    {
        "caption": "TouchViz",
        "img": "researchImages/touchvis.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Fisher, D., Sadana, R., and Herron, J. (2013). TouchViz: a case study comparing two interfaces for data analytics on tablets. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 2301â2310). ACM.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Sadana",
                "Herron",
                "schraefel"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Touch"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/touchvis-CHI2013-cameraready.pdf",
        "video": "http://research.microsoft.com/en-us/um/people/sdrucker/video/panodata.mp4.mp4",
        "abstract": "As more applications move from the desktop to touch devices like tablets, designers must wrestle with the costs of porting a design with as little revision of the UI as possible from one device to the other, or of optimizing the interaction per device. We consider the tradeoffs between two versions of a UI for working with data on a touch tablet. One interface is based on using the conventional desktop metaphor (WIMP) with a control panel, push buttons, and checkboxes where the mouse click is effectively replaced by a finger tap. The other interface (which we call FLUID) eliminates the control panel and focuses touch actions on the data visualization itself. We describe our design process and evaluation of each interface. We discuss the significantly better task performance and preference for the FLUID interface, in particular how touch design may challenge certain assumptions about the performance benefits of WIMP interfaces that do not hold on touch devices, such as the superiority of gestural vs. control panel based interaction.",
        "primary": "Visualization"
    },
    {
        "caption": "Cinema Program",
        "img": "researchImages/cinema.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Galyean, T. A., and Zeltzer, D. (1992). Cinema: A system for procedural camera movements. In <i>Proceedings of the 1992 symposium on Interactive 3D graphics</i> (pp. 67â70). ACM.</div>",
        "tags": {
            "collaborators": [
                "Galyean",
                "Zeltzer"
            ],
            "subject": [
                "Graphics",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1992"
            ],
            "publication": [
                "SIGGRAPH",
                "Thesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/SIG92symp.pdf",
        "abstract": "This paper presents a general system for camera movement upon which a wide variety of higher-level methods and applications can be built. In addition to the basic commands for camera placement, a key attribute of the CINEMA system is the ability to inquire information directly about the 3D world through which the camera is moving. With this information high-level procedures can be written that closely correspond to more natural camera specifications. Examples of some high-level procedures are presented. In addition, methods for overcoming deficiencies of this procedural approach are proposed.",
        "primary": "Camera"
    },
    {
        "caption": "Smart Skip",
        "img": "researchImages/smartskip_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Glatzer, A., De Mar, S., and Wong, C. (2002). SmartSkip: consumer level browsing and skipping of digital video content. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems</i> (pp. 219â226). ACM.</div>",
        "tags": {
            "collaborators": [
                "Roseway",
                "De Mar",
                "Wong"
            ],
            "subject": [
                "UI",
                "TV",
                "Media"
            ],
            "year": [
                "2002"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/smartskipfinal.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/smartskip2.wmv",
        "abstract": "In this paper, we describe an interface for browsing and skipping digital video content in a consumer setting; that is, sitting and watching television from a couch using a standard remote control. We compare this interface with two other interfaces that are in common use today and found that subjective satisfaction was statistically better with the new interface. Performance metrics however, like time to task completion and number of clicks were worse.",
        "primary": "Media"
    },
    {
        "caption": "Powerpoint Diff",
        "img": "researchImages/vizpptdiff.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Petschnigg, G., and Agrawala, M. (2006). Comparing and managing multiple versions of slide presentations. In <i>Proceedings of the 19th annual ACM symposium on User interface software and technology</i> (pp. 47â56). ACM.</div>",
        "tags": {
            "collaborators": [
                "Petschnigg",
                "Agrawala"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Presentation",
                "Temporal"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/fp214-DruckerFinalSmall.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/pptviznew.wmv",
        "abstract": "Despite the ubiquity of slide presentations, managing multiple presentations remains a challenge. Understanding how multiple versions of a presentation are related to one another, assembling new presentations from existing presentations, and collaborating to create and edit presentations are difficult tasks. <br>  In this paper, we explore techniques for comparing and managing multiple slide presentations. We propose a general comparison framework for computing similarities and differences between slides. Based on this framework we develop an interactive tool for visually comparing multiple presentations. The interactive visualization facilitates understanding how presentations have evolved over time. We show how the interactive tool can be used to assemble new presentations from a collection of older ones and to merge changes from multiple presentation authors.",
        "primary": "Visualization"
    },
    {
        "caption": "Visual Decision Maker",
        "img": "researchImages/VDM.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Regan, T., Roseway, A., and Lofstrom, M. (2005). The visual decision maker: A recommendation system for collocated users. In <i>Proceedings of the 2005 conference on Designing for User eXperience</i> (p. 21). AIGA: American Institute of Graphic Arts.</div>",
        "tags": {
            "collaborators": [
                "Regan",
                "Roseway",
                "Lofstrom"
            ],
            "subject": [
                "Graphics",
                "UI",
                "Movies",
                "Visualization"
            ],
            "year": [
                "2005"
            ],
            "publication": [
                "DUX"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/vdmfinal.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/vdm.wmv",
        "abstract": "We present the Visual Decision Maker (VDM), an application that gives movie recommendations to groups of people sitting together. The VDM provides a TV like user experience: a stream of movie stills flows towards the center of the screen, and users press buttons on remote controls to vote on the currently selected movie. A collaborative filtering engine provides recommendations for each user and for the group as a whole based on the votes. Three principles guided our design of the VDM: shared focus, dynamic pacing, and encouraging conversations. In this paper we present the results of a four month public installation and a lab study showing how these design choices affected people's usage and people's experience of the VDM. Our results show that shared focus is important for users to feel that the group's tastes are represented in the recommendations.",
        "primary": "Media"
    },
    {
        "caption": "Parallel Radiosity",
        "img": "researchImages/parallelRadiosity.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and SchrÃ¶der, P. (1992). Fast radiosity using a data parallel architecture. In <i>Third Eurographics Workshop on Rendering (Bristol, uk</i> (pp. 247â258).</div>",
        "tags": {
            "collaborators": [
                "Schroeder"
            ],
            "subject": [
                "Graphics",
                "Parallel Computing"
            ],
            "year": [
                "1992"
            ],
            "publication": [
                "PhotorealisticWorkshop"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/eurorendworkshop.pdf",
        "abstract": "We present a data parallel algorithm for radiosity. The algorithm was designed to take advantage of large numbers of processors. It has been implemented on the Connection Machine CM2 system and scales linearly in the number of available processors over a wide range. All parts of the algorithm | form-factor computation, visibility determination, adaptive subdivision, and linear algebra solution | execute in parallel with a completely distributed database. Load balancing is achieved through processor allocation and dynamic data structures which reconfigure appropriately to match the granularity of the required calculations.",
        "primary": "Graphics"
    },
    {
        "caption": "Token TV",
        "img": "researchImages/tokenTV_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Wong, C. (2001b). <i>Token TV: Sharing preferences for Television DVR recording</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Wong"
            ],
            "subject": [
                "UI",
                "TV",
                "Media"
            ],
            "year": [
                "2001"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/tokentv.pdf",
        "abstract": "TV Tokens (GUID for a specific broadcast program or movie) can be embedded in any website, EPG or email, downloaded and shared between friends to send to respective PVR's to schedule recording of show. TokenTV service (dot.NET TV) converts GUID to resolve to local schedule information needed to program the PVR. Any content based website (i.e.: IBDB.com, PBS.org, AFI.org) could have tokens to download to PVR for recording specific content.",
        "primary": "Media"
    },
    {
        "caption": "DeepNews",
        "img": "researchImages/deepnews_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Wong, C. (2001a). <i>DeepNews: Automatic related material based on closed caption information</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Wong"
            ],
            "subject": [
                "UI",
                "TV",
                "Media"
            ],
            "year": [
                "2001"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "video": "http://research.microsoft.com/~sdrucker/Video/DeepnewsEnhanced.wmv",
        "abstract": "By monitoring the closed caption stream of a news broadcast, the web can be searched for related articles and more in depth stories can be found.",
        "primary": "Media"
    },
    {
        "caption": "Filtered EPG",
        "img": "researchImages/tvnow_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Wong, C. (2002). <i>Filtered Electronic Program Guides</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Wong",
                "Roseway"
            ],
            "subject": [
                "UI",
                "TV",
                "Media"
            ],
            "year": [
                "2002"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "video": "http://research.microsoft.com/~sdrucker/nextmedia/TVNow.exe",
        "abstract": "This electronic program guide uses automatically computed favorites based on viewing habits per time of day and day of week as well as simple filtering features to allow for rapid selection of television.",
        "primary": "Media"
    },
    {
        "caption": "People Browser",
        "img": "researchImages/peoplebrowser_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Wong, C. (2003b). <i>People Browser</i>.</div>",
        "tags": {
            "collaborators": [
                "Wong"
            ],
            "subject": [
                "UI",
                "Visualization",
                "Information"
            ],
            "year": [
                "2003"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "video": "http://research.microsoft.com/~sdrucker/Video/peoplebrowser.avi",
        "abstract": "The concept of 6 degrees separation (6 DOS) can be applied to many different domains. As demonstrated in the MediaVariations Browser, movies can easily be browsed using clusters of related movies using the actor and director to help associate movies. Looking at people is even more natural for this type of browsing, since that is what the concept of 6 DOS is usually associated with. The PeopleBrowser uses a person's rank within an organization, their management chain, their peers (under the same manager), their direct reports, and people with their same title, to help browse through an organization. Other clusters could also easily be used, including those people on the same mailing list, frequently mailed, etc. This project was done in conjunction with the Shell MSX team (Hillel Cooperman, Rob Girling, and Jeni Sadler).",
        "primary": "UI-Information"
    },
    {
        "caption": "Movie Variations",
        "img": "researchImages/MV_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Wong, C. (2003a). <i>Movie Variations</i>.</div>",
        "tags": {
            "collaborators": [
                "Roseway",
                "De Mar",
                "Wong"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Movies"
            ],
            "year": [
                "2003"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "video": "http://research.microsoft.com/~sdrucker/Video/mbrowse2.wmv",
        "abstract": "This system allows for browsing a movie collection by moving from one related group of movies to another related group, where groups are related by common actor or director. As the user selects a movie from the cluster, it moves to the center and 4 related clusters are moved arranged around the movie. Extensions can include clusters that are related by collaborative filtering or other common features.",
        "primary": "Media"
    },
    {
        "caption": "Right Now Viewer",
        "img": "researchImages/rightnow_viewer_photo.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Wong, C., and De Mar, S. (2002). <i>Right Now Viewer</i>.</div>",
        "tags": {
            "collaborators": [
                "Wong",
                "Flora"
            ],
            "subject": [
                "UI",
                "TV"
            ],
            "year": [
                "2002"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "abstract": "The purpose of this thought experiment was to look at time compression for when you turned on the TV to quickly find out what's on. Changing channels takes time and often there's a commercial on so you have to wait. This demo shows how a tuner could cache 12 most popular tv channels you watch and assemble them into a time compressed 30x real time video clip. In this prototype you can click on an individual thumbnail and it plays regular speed. The time compression inherent in the clips is long enough to transcend the commercials so you can see what's on. UI study participants were able to distinguish different TV formats (like sports vs. news), but failed to get a more detailed grasp of the program. For this the UI seemed to display too much information simultaneously. Additional experiments need to be done to find the right balance of speed, number and size of simultaneous videos playing and video thumbnail size would improve comprehension and recognition.",
        "primary": "Media"
    },
    {
        "caption": "MediaBrowser",
        "img": "researchImages/MFAG.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., Wong, C., Roseway, A., Glenner, S., and De Mar, S. (2004). MediaBrowser: reclaiming the shoebox. In <i>Proceedings of the working conference on Advanced visual interfaces</i> (pp. 433â436). ACM.</div>",
        "tags": {
            "collaborators": [
                "Wong",
                "Roseway",
                "Glenner",
                "De Mar"
            ],
            "subject": [
                "Graphics",
                "Photos",
                "UI",
                "Visualization"
            ],
            "year": [
                "2004"
            ],
            "publication": [
                "AVI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/mediaframeAVIlong.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/LH%20MediaFrame%20Final.wmv",
        "abstract": "Applying personal keywords to images and video clips makes it possible to organize and retrieve them, as well as automatically create thematically related slideshows. MediaBrowser is a system designed to help users create annotations by uniting a careful choice of interface elements, an elegant and pleasing design, smooth motion and animation, and a few simple tools that are predictable and consistent. The result is a friendly, useable tool for turning shoeboxes of old photos into labeled collections that can be easily browsed, shared, and enjoyed.",
        "primary": "UI-Information"
    },
    {
        "caption": "Virtual Museum",
        "img": "researchImages/museum1.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Zeltzer, D. (1994). Intelligent camera control in a virtual environment. In <i>Graphics Interface</i> (pp. 190â190). CANADIAN INFORMATION PROCESSING SOCIETY.</div>",
        "tags": {
            "collaborators": [
                "Zeltzer"
            ],
            "subject": [
                "Graphics",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1994"
            ],
            "publication": [
                "GI",
                "Thesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/GImuseum_wfigs.pdf",
        "abstract": "This paper describes a framework for exploring intelligent camera controls in a 3D virtual environment. It presents a methodology for designing the underlying camera controls based on an analysis of what tasks are to be required in a specific environment. Once an underlying camera framework is built, a variety of interfaces can be connected to the framework. A virtual museum is used as a prototypical virtual environment for this work. This paper identifies some of the tasks that need to be performed in a virtual museum; presents a paradigm for encapsulating those tasks into camera modules; and describes in detail the underlying mechanisms that make up the camera module for navigating through the environment.",
        "primary": "Camera"
    },
    {
        "caption": "SandDance",
        "img": "researchImages/sanddance.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Fernandez, R. (2013). <i>SandDance</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Fernandez",
                "Fisher"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Touch"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/sanddance.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/TouchViz2.mp4",
        "abstract": "SandDance is a browser based information visualization system prototype created at Microsoft Research that scales to hundreds of thousands of items. Arbitrary datatables can be loaded and results can be filtered using facets and search and displayed in a variety of layouts. Transitions between the views are animated so that users can better maintain context. Multiple linked views allow for associations between the same items in each view. Multiple devices can simultaneusly interact with each other on the same dataset.  Using principles of information visualization, users can map any attribute into the position, color, size, opacity and layout of a dataset to help reveal patterns within the data. SandDance lets you see both the individual records, and their overall structure.  SandDance focusses on natural user interaction techniques. Touch interaction is a first class citizen, allowing the entire experience to be easily operated through a touch screen. The system also understand speech commands for searching, selecting, focusing and filtering the data. A kinect system can be used to sense gestures for moving between views of the data. Collaboration is supported by allowing multiple sets of people to interact with the same dataset. Selections and filters in one system are automatically replicated to other systems viewing the data.",
        "primary": "Visualization"
    },
    {
        "caption": "LiveLabs Thumbtack",
        "img": "researchImages/thumbtackthumb.jpg",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M., and Hydrie, A. (2009). <i>LiveLabs ThumbTack</i>.</div>",
        "tags": {
            "collaborators": [
                "Hydrie",
                "Cutler",
                "Oliveira",
                "Bergeron",
                "Lakshmiratan"
            ],
            "subject": [
                "UI",
                "Information",
                "Web"
            ],
            "year": [
                "2009"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "abstract": "Thumbtack is an easy way to save links, photos, and anything else you can find on bunch of different Web sites to a single place.  Grab the stuff you want, put it into a Thumbtack collection, then get to it from anywhere you can get online.  Share it with your friends, or just keep it for yourself. It's way easier than sending a bunch of links in an e-mail, and even easier than setting lots of favorites in your browser.",
        "primary": "UI-Information"
    },
    {
        "caption": "CamDroid",
        "img": "researchImages/camdroid.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M. (1994a). CamDroid. In <i>Intelligent camera control for graphical environments</i>. MIT.</div>",
        "tags": {
            "collaborators": [
                "Zeltzer"
            ],
            "subject": [
                "Graphics",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1995"
            ],
            "publication": [
                "SIGGRAPH",
                "Thesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/SIG95symp.pdf",
        "abstract": "In this paper, a method of encapsulating camera tasks into well defined units called camera modules is described. Through this encapsulation, camera modules can be programmed and sequenced, and thus can be used as the underlying framework for controlling the virtual camera in widely disparate types of graphical environments. Two examples of the camera framework are shown: an agent which can film a conversation between two virtual actors and a visual programming language for filming a virtual football game.",
        "primary": "Camera"
    },
    {
        "caption": "Virtual Football",
        "img": "researchImages/footballsm.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M. (1994c). Virtual Football. In <i>Intelligent camera control for graphical environments</i>. MIT.</div>",
        "tags": {
            "collaborators": [
                "Individual"
            ],
            "subject": [
                "Graphics",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1994"
            ],
            "publication": [
                "Thesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/thesiswbmakrs.pdf",
        "abstract": "Too often in the field of computer graphics, practitioners have been more concerned with the question of how to move a camera rather than why to move it. This thesis addresses the core question of why the camera is being placed and moved and uses answers to that question to provide a more convenient, more intelligent method for controlling virtual cameras in computer graphics. After discussing the general sorts of activities to be performed in graphical environments, this thesis then contains a derivation of some camera primitives that are required, and examines how they can be incorporated into different interfaces. A single, consistent, underlying framework for camera control across many different domains has been posited and formulated in terms of constrained optimization. Examples from different application domains demonstrate a variety of interface styles that have all been implemented on top of the underlying framework. Evaluations for each application are also given.",
        "primary": "Camera"
    },
    {
        "caption": "Conversation Agent",
        "img": "researchImages/conversesm.gif",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S. M. (1994b). <i>Intelligent camera control for graphical environments</i>. Massachusetts Institute of Technology.</div>",
        "tags": {
            "collaborators": [
                "Individual"
            ],
            "subject": [
                "Graphics",
                "Animation",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1994"
            ],
            "publication": [
                "Camera"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/thesiswbmakrs.pdf",
        "abstract": "Too often in the field of computer graphics, practitioners have been more concerned with the question of how to move a camera rather than why to move it. This thesis addresses the core question of why the camera is being placed and moved and uses answers to that question to provide a more convenient, more intelligent method for controlling virtual cameras in computer graphics. After discussing the general sorts of activities to be performed in graphical environments, this thesis then contains a derivation of some camera primitives that are required, and examines how they can be incorporated into different interfaces. A single, consistent, underlying framework for camera control across many different domains has been posited and formulated in terms of constrained optimization. Examples from different application domains demonstrate a variety of interface styles that have all been implemented on top of the underlying framework. Evaluations for each application are also given.",
        "primary": "Camera"
    },
    {
        "caption": "SlideSpace",
        "img": "researchImages/slidespace.png",
        "bibEntry": "<div class=\"csl-entry\">Edge, D., Yang, X., Kotturi, Y., Wang, S., Feng, D., Lee, B., and Drucker, S. (2016). SlideSpace: Heuristic Design of a Hybrid Presentation Medium. <i>ACM Transactions on Computer-Human Interaction (TOCHI)</i>, <i>23</i>(3), 16.</div>",
        "tags": {
            "collaborators": [
                "Edge",
                "Yang",
                "Kotturi",
                "Wang",
                "Feng",
                "Lee"
            ],
            "subject": [
                "UI",
                "Presentation",
                "Information"
            ],
            "year": [
                "2016"
            ],
            "publication": [
                "TOCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/a16-edge.pdf",
        "abstract": "The Slide and Canvas metaphors are two ways of helping people create visual aids for oral presentations. Although such physical metaphors help both authors and audiences make sense of material, they also constrain authoring in ways that can negatively impact presentation delivery. In this article, we derive heuristics for the design of presentation media that are independent of any underlying physical metaphors. We use these heuristics to craft a new kind of presentation medium called SlideSpace?one that combines hierarchical outlines, content collections, and design rules to automate the real-time, outline-driven synthesis of hybrid Slide-Canvas visuals. Through a qualitative study of SlideSpace use, we validate our heuristics and demonstrate that such a hybrid presentation medium can combine the advantages of existing systems while mitigating their drawbacks. Overall, we show how a heuristic design approach helped us challenge entrenched physical metaphors to create a fundamentally digital presentation medium with the potential to transform the activities of authoring, delivering, and viewing presentations.",
        "primary": "Presentation"
    },
    {
        "caption": "Dynamic Web",
        "img": "researchImages/instrumenting.png",
        "bibEntry": "<div class=\"csl-entry\">Edmonds, A., White, R. W., Morris, D., and Drucker, S. M. (2007). Instrumenting the dynamic web. <i>Journal of Web Engineering</i>, <i>6</i>(3), 243.</div>",
        "tags": {
            "collaborators": [
                "Edmonds",
                "White",
                "Morris"
            ],
            "subject": [
                "Information"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "JWE"
            ]
        },
        "primary": "UI-Information"
    },
    {
        "caption": "Big Data Interaction",
        "img": "researchImages/bigdata.png",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., DeLine, R., Czerwinski, M., and Drucker, S. (2012). Interactions with big data analytics. <i>Interactions</i>, <i>19</i>(3), 50â59.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "DeLine",
                "Czerwinski"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Big Data"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "Interactions"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/inteactions_big_data.pdf",
        "video": "http://research.microsoft.com/apps/pubs/default.aspx?id=217732",
        "abstract": "Increasingly in the 21st century,  our daily lives leave behind a  detailed digital record: our shifting  thoughts and opinions shared on  Twitter, our social relationships, our purchasing habits, our information seeking, our photos and videos - even the movements of our bodies and cars",
        "primary": "Visualization"
    },
    {
        "caption": "Logan",
        "img": "researchImages/Logan.PNG",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., DeLine, R., Czerwinski, M., and Drucker, S. (2016). <i>Understanding the Breadth of the Event Space: Learning from Logan</i>.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "DeLine",
                "Czerwinski"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Sequences"
            ],
            "year": [
                "2016"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/EVENT_2016_paper_17.pdf",
        "abstract": "Event processing, analysis, and visualization are increasingly important, and common, problems as telemetry and log recording become ubiquitous. We are still learning about the space of ways to both query and see the results of sequential queries against event-logged data. In this paper, we discuss our Logan event exploration prototype, which is based on regular-expression queries and result histograms; we then discuss the many factors that vary between tools, domains, and datasets ? tradeoffs of factors such as session size, cardinality of events, and the presence of event arguments.",
        "primary": "Visualization"
    },
    {
        "caption": "Bi Analytics",
        "img": "researchImages/BI.PNG",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., Drucker, S., and Czerwinski, M. (2014). Business Intelligence Analytics [Guest editorsâ introduction]. <i>IEEE Computer Graphics and Applications</i>, <i>34</i>(5), 22â24.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Drucker"
            ],
            "subject": [
                "Information",
                "Visualization"
            ],
            "year": [
                "2015"
            ],
            "publication": [
                "CGA"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/mcg2014050022.pdf",
        "abstract": "Businesses are increasingly monitoring and tracking data about what it takes to keep themselves running. They collect and maintain increasingly available data, such as ? transaction and sales data stored in data warehouses, ? server log files tracking visitors, ? data from sensors tracking delays on factory floors, ? IT data logs, and ? data on their competitors and industrial sectors. Data-driven decision making?orienting business decisions around data?drives major IT initiatives across all business sectors",
        "primary": "UI-Information"
    },
    {
        "caption": "Vis-a-vis",
        "img": "researchImages/visavis.png",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., Drucker, S., Fernandez, R., and Chen, X. (2011). <i>Vis-a-vis: A Visual Language for Spreadsheet Visualizations</i>. Technical Report MSR-TR-2011-142, Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Fernandez",
                "Chen"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Design"
            ],
            "year": [
                "2011"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/VisAVis.pdf",
        "video": "",
        "abstract": "Finding ways for information workers to easily create and modify visualizations that display their own data has been a long time goal within the visualization community. We describe Vis-a-vis, a declarative language for defining and extending visualizations directly within spreadsheets. Vis-a-vis allows users to directly bind data and formula to the visual attributes of an extensible set of visualization primitives. The visualizations that Vis-a-vis creates can be shared and modified easily, allowing users to modify existing visualizations. This approach allows users to select visualizations from a gallery, to customize them easily, or to create novel visualizations. The approach leverages familiar formulas and data from spreadsheets. We prototype a system that uses this language, and use it to build a number of standard and custom visualizations, and gather formative feedback from a small user study.",
        "primary": "Visualization"
    },
    {
        "caption": "WebCharts",
        "img": "researchImages/webcharts.png",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., Drucker, S., Fernandez, R., and Ruble, S. (2010). Visualizations everywhere: A multiplatform infrastructure for linked visualizations. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>16</i>(6), 1157â1163.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Fernandez",
                "Ruble"
            ],
            "subject": [
                "UI",
                "Visualization"
            ],
            "year": [
                "2010"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/webcharts.pdf",
        "video": "",
        "abstract": "In order to use new visualizations, most toolkits require application developers to rebuild their applications and distribute new versions to users. The WebCharts Framework take a different approach by hosting Javascript from within an application and providing a standard data and events interchange.. In this way, applications can be extended dynamically, with a wide variety of visualizations. We discuss the benefits of this architectural approach, contrast it to existing techniques, and give a variety of examples and extensions of the basic system.",
        "primary": "Visualization"
    },
    {
        "caption": "IncrementalVis",
        "img": "researchImages/incrvis.png",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., Drucker, S. M., and KÃ¶nig, A. C. (2012). Exploratory visualization involving incremental, approximate database queries and uncertainty. <i>IEEE Computer Graphics and Applications</i>, <i>32</i>(4), 55â62.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Konig"
            ],
            "subject": [
                "Visualization"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "CGA"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/Exploratory_CGA.pdf",
        "video": "",
        "abstract": "Large datasets can mean slow queries, for which users must wait. Incremental visualization systems can give faster results at a cost of accuracy. This article asked analysts to use one and report on their results. Their feedback provides suggestions for alternative visualizations to represent a query still in progress.",
        "primary": "Visualization"
    },
    {
        "caption": "Incremental Visualization",
        "img": "researchImages/incvis.png",
        "bibEntry": "<div class=\"csl-entry\">Fisher, D., Popov, I., Drucker, S., and schraefel,  m. c. (2012). Trust me, Iâm partially right: incremental visualization lets analysts explore large datasets faster. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 1673â1682). ACM.</div>",
        "tags": {
            "collaborators": [
                "Fisher",
                "Popov",
                "schraefel"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Big Data"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/chi2012_interactive.pdf",
        "video": "",
        "abstract": "Queries over large scale (petabyte) data bases often mean waiting overnight for a result to come back. Scale costs  time. Such time also means that potential avenues of  exploration are ignored because the costs are perceived to  be too high to run or even propose them. With  sampleAction we have explored whether interaction  techniques to present query results running over only  incremental samples can be presented as sufficiently  trustworthy for analysts both to make closer to real time  decisions about their queries and to be more exploratory in  their questions of the data. Our work with three teams of  analysts suggests that we can indeed accelerate and open up  the query process with such incremental visualizations.",
        "primary": "Visualization"
    },
    {
        "caption": "LiveLabs Pivot Viewer",
        "img": "researchImages/pivot.png",
        "bibEntry": "<div class=\"csl-entry\">Flake, G., Farouki, K., Brewer, B., and Drucker, S. M. (2009). <i>LiveLabs Pivot Viewer</i>.</div>",
        "tags": {
            "collaborators": [
                "Flake",
                "Brewer"
            ],
            "subject": [
                "UI",
                "Information",
                "Web"
            ],
            "year": [
                "2009"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "video": "http://research.microsoft.com/~sdrucker/video/ThumbtackFinal/ThumbtackIntroductionVideo.wmv",
        "abstract": "Pivot is an experimental application for exploring large data sets with smooth visual interactions. The application originally was released by Microsoft Live Labs in October 2009, and it is being re-released by Microsoft Research to enable the research community to continue to use it for experiments. If you have Internet Explorer 9 installed, disable GPU rendering in Internet Explorer to enable Pivot to work correctly. The Pivot collection home page points to content no longer available, but Pivot still can be used for viewing user-created local or web collections. This standalone version of Pivot is unsupported and might stop functioning properly in the future.",
        "primary": "Visualization"
    },
    {
        "caption": "Self Inflated",
        "img": "researchImages/sismall.gif",
        "bibEntry": "<div class=\"csl-entry\">Galyean, T., and Drucker, S. (1992). <i>Self Inflated</i>.</div>",
        "tags": {
            "collaborators": [
                "Galyean"
            ],
            "subject": [
                "Graphics"
            ],
            "year": [
                "1992"
            ],
            "publication": [
                "Movie"
            ]
        },
        "abstract": "A video about a republican challenger in 1992. Shown at the Democratic National Convention.",
        "primary": "Graphics"
    },
    {
        "caption": "Memex Vision",
        "img": "researchImages/memex1.png",
        "bibEntry": "<div class=\"csl-entry\">Gemmell, J., Bell, G., Lueder, R., Drucker, S., and Wong, C. (2002). MyLifeBits: fulfilling the Memex vision. In <i>Proceedings of the tenth ACM international conference on Multimedia</i> (pp. 235â238). ACM.</div>",
        "tags": {
            "collaborators": [
                "Gemmell",
                "Bell",
                "Lueder",
                "Wong"
            ],
            "subject": [
                "UI",
                "Media",
                "Search",
                "Information"
            ],
            "year": [
                "2002"
            ],
            "publication": [
                "Multimedia"
            ]
        },
        "abstract": "MyLifeBits is a project to fulfill the Memex vision first posited by Vannevar Bush in 1945. It is a system for storing all of one???s digital media, including documents, images, sounds, and videos. It is built on four principles: (1) collections and search must replace hierarchy for organization (2) many visualizations should be supported (3) annotations are critical to non-text media and must be made easy, and (4) authoring should be via transclusion.",
        "primary": "UI-Information"
    },
    {
        "caption": "Foveated",
        "img": "researchImages/foveated.png",
        "bibEntry": "<div class=\"csl-entry\">Guenter, B., Finch, M., Drucker, S., Tan, D., and Snyder, J. (2012). Foveated 3D graphics. <i>ACM Transactions on Graphics (TOG)</i>, <i>31</i>(6), 164.</div>",
        "tags": {
            "collaborators": [
                "Guenter",
                "Finch",
                "Tan",
                "Snyder"
            ],
            "subject": [
                "Graphics",
                "Photos"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "SIGGRAPHAsia"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/userstudy07.pdf",
        "abstract": "We present a data-driven method to predict the performance of an image completion method. Our image completion method is based on the state-of-the-art non-parametric framework of Wexler et al. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape before the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm.",
        "primary": "Graphics"
    },
    {
        "caption": "FaceRec",
        "img": "researchImages/facerec.png",
        "bibEntry": "<div class=\"csl-entry\">Hua, G., Viola, P. A., and Drucker, S. M. (2007). Face recognition using discriminatively trained orthogonal rank one tensor projections. In <i>2007 IEEE Conference on Computer Vision and Pattern Recognition</i> (pp. 1â8). IEEE.</div>",
        "tags": {
            "collaborators": [
                "Hua",
                "Viola"
            ],
            "subject": [
                "Machine Learning"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "CVPR"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/CVPR07a.pdf",
        "abstract": "We propose a method for face recognition based on a discriminative linear projection. In this formulation images are treated as tensors, rather than the more conventional vector of pixels. Projections are pursued sequentially and take the form of a rank one tensor, i.e., a tensor which is the outer product of a set of vectors. A novel and effective technique is proposed to ensure that the rank one tensor projections are orthogonal to one another. These constraints on the tensor projections provide a strong inductive bias and result in better generalization on small training sets. Our work is related to spectrum methods, which achieve orthogonal rank one projections by pursuing consecutive projections in the complement space of previous projections. Although this may be meaningful for applications such as reconstruction, it is less meaningful for pursuing discriminant projections. Our new scheme iteratively solves an eigenvalue problem with orthogonality constraints on one dimension, and solves unconstrained eigenvalue problems on the other dimensions. Experiments demonstrate that on small and medium sized face recognition datasets, this approach outperforms previous embedding methods. On large face datasets this approach achieves results comparable with the best, often using fewer discriminant projections.",
        "primary": "Machine Learning"
    },
    {
        "caption": "Narrative",
        "img": "researchImages/narratives.png",
        "bibEntry": "<div class=\"csl-entry\">Hullman, J., Drucker, S., Riche, N. H., Lee, B., Fisher, D., and Adar, E. (2013). A deeper understanding of sequence in narrative visualization. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>19</i>(12), 2406â2415.</div>",
        "tags": {
            "collaborators": [
                "Hullman",
                "Riche",
                "Fisher",
                "Adar",
                "Lee"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Presentation"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/narrative.pdf",
        "video": "http://research.microsoft.com/apps/video/default.aspx?id=188294",
        "abstract": "Conveying a narrative with visualizations often requires choosing an order in which to present visualizations. While evidence exists that narrative sequencing in traditional stories can affect comprehension and memory, little is known about how sequencing choices affect narrative visualization. We consider the forms and reactions to sequencing in narrative visualization presentations to provide a deeper understanding with a focus on linear, slideshow-style presentations. We conduct a qualitative analysis of 42 professional narrative visualizations to gain empirical knowledge on the forms that structure and sequence take. Based on the results of this study we propose a graph-driven approach for automatically identifying effective sequences in a set of visualizations to be presented linearly. Our approach identifies possible transitions in a visualization set and prioritizes local (visualization-to-visualization) transitions based on an objective function that minimizes the cost of transitions from the audience perspective. We conduct two studies to validate this function. We also expand the approach with additional knowledge of user preferences for different types of local transitions and the effects of global sequencing strategies on memory, preference, and comprehension. Our results include a relative ranking of types of visualization transitions by the audience perspective and support or memory and subjective rating benefits of visualization sequences that use parallelism as a structural device. We discuss how these insights can guide the design of narrative visualization and systems that support optimization of visualization sequence.",
        "primary": "Visualization"
    },
    {
        "caption": "TimeQuilt",
        "img": "researchImages/tq.png",
        "bibEntry": "<div class=\"csl-entry\">Huynh, D. F., Drucker, S. M., Baudisch, P., and Wong, C. (2005). Time quilt: scaling up zoomable photo browsers for large, unstructured photo collections. In <i>CHIâ05 extended abstracts on Human factors in computing systems</i> (pp. 1937â1940). ACM.</div>",
        "tags": {
            "collaborators": [
                "Huynh",
                "Baudisch",
                "Wong"
            ],
            "subject": [
                "UI",
                "Photos",
                "Visualization"
            ],
            "year": [
                "2005"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/CHI2005%20-%20Time%20Quilt%20short.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/timequilt.wmv",
        "abstract": "In the absence of manual organization of large digital photo collections, the photos' visual content and creation dates can help support time-based visual search tasks. Current zoomable photo browsers are designed to support visual searches by maximizing screenspace usage. However, their space-filling layouts fail to convey temporal order effectively. We propose a novel layout called time quilt that trades off screenspace usage for better presentation of temporal order. In an experimental comparison of space-filling, linear timeline, and time quilt layouts, participants carried out the task of finding photos in their personal photo collections averaging 4,000 items. They performed 45% faster on time quilt. Furthermore, while current zoomable photo browsers are designed for visual searches, this support does not scale to thousands of photos: individual thumbnails become less informative as they grow smaller. We found a subjective preference for the use of representative photos to provide an overview for visual searches in place of the diminishing thumbnails.",
        "primary": "Photos"
    },
    {
        "caption": "Imageflow",
        "img": "researchImages/imageflow.png",
        "bibEntry": "<div class=\"csl-entry\">Jampani, V., Ramos, G., and Drucker, S. (2010). <i>ImageFlow: Streaming Image Search</i>. Microsoft Research.</div>",
        "tags": {
            "collaborators": [
                "Ramos",
                "Jampani"
            ],
            "subject": [
                "Media",
                "UI",
                "Web",
                "Visualization"
            ],
            "year": [
                "2010"
            ],
            "publication": [
                "TechReport"
            ]
        },
        "abstract": "{Traditional grid and list representations of image search results are the dominant interaction paradigms that users face on a daily basis, yet it is unclear that such paradigms are well-suited for experiences where the user's task is to browse images for leisure, to discover new information or to seek particular images to represent ideas. We introduce ImageFlow, a novel image search user interface that explores a different alternative to the traditional presentation of image search results. ImageFlow presents image results on a canvas where we map semantic features (e.g., relevance, related queries) to the canvas' spatial dimensions (e.g., x, y, z) in a way that allows for several levels of engagement ÃÃÂ¢ÃÃ from passively viewing a stream of images, to seamlessly navigating through the semantic space and actively collecting images for sharing and reuse. We have implemented our system as a fully functioning prototype, and we report on promising, preliminary usage results.},",
        "primary": "UI-Information"
    },
    {
        "caption": "Social Dilemma Testing",
        "img": "researchImages/socdilemma.gif",
        "bibEntry": "<div class=\"csl-entry\">Jensen, C., Farnham, S. D., Drucker, S. M., and Kollock, P. (2000). The effect of communication modality on cooperation in online environments. In <i>Proceedings of the SIGCHI conference on Human Factors in Computing Systems</i> (pp. 470â477). ACM.</div>",
        "tags": {
            "collaborators": [
                "Jensen",
                "Farnham",
                "Kollock"
            ],
            "subject": [
                "Social"
            ],
            "year": [
                "2000"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/chidilemmas.pdf",
        "abstract": "One of the most robust findings in the sociological literature is the positive effect of communication on cooperation and trust.  When individuals are able to communicate, cooperation increases significantly.  How does the choice of communication modality influence this effect?  We adapt the social dilemma research paradigm to quantitatively analyze different modes of communication. Using this method, we compare four forms of communication: no communication, text-chat, text-to-speech, and voice.  We found statistically significant differences between the various forms of communication, with the voice condition resulting in the highest levels of cooperation.  Our results highlight the importance of striving towards the use of more advanced forms of communication in online environments, especially where trust and cooperation are essential.  In addition, our research demonstrates the applicability of the social dilemma paradigm in testing the extent to which communication modalities promote the development of trust and cooperation.",
        "primary": "Social"
    },
    {
        "caption": "Cliplets",
        "img": "researchImages/cliplets.png",
        "bibEntry": "<div class=\"csl-entry\">Joshi, N., Mehta, S., Drucker, S., Stollnitz, E., Hoppe, H., Uyttendaele, M., and Cohen, M. (2012). Cliplets: juxtaposing still and dynamic imagery. In <i>Proceedings of the 25th annual ACM symposium on User interface software and technology</i> (pp. 251â260). ACM.</div>",
        "tags": {
            "collaborators": [
                "Joshi",
                "Metha",
                "Stollnitz",
                "Cohen",
                "Hoppe",
                "Uyttendaele"
            ],
            "subject": [
                "UI",
                "Graphics",
                "Photos"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/paper_uist_final.pdf",
        "abstract": "We explore creating cliplets, a form of visual media that juxtaposes still image and video segments, both spatially and temporally, to expressively abstract a moment. Much as in cinemagraphs, the tension between static and dynamic elements in a cliplet reinforces both aspects, strongly focusing the viewer's attention. Creating this type of imagery is challenging without professional tools and training. We develop a set of idioms, essentially spatiotemporal mappings, that characterize cliplet elements, and use these idioms in an interactive system to quickly compose a cliplet from ordinary handheld video. One difficulty is to avoid artifacts in the cliplet composition without resorting to extensive manual input. We address this with automatic alignment, looping optimization and feathering, simultaneous matting and compositing, and Laplacian blending. A key user-interface challenge is to provide affordances to define the parameters of the mappings from input time to output time while maintaining a focus on the cliplet being created. We demonstrate the creation of a variety of cliplet types. We also report on informal feedback as well as a more structured survey of users.",
        "primary": "Photos"
    },
    {
        "caption": "Refinery",
        "img": "researchImages/refinery.png",
        "bibEntry": "<div class=\"csl-entry\">Kairam, S., Riche, N. H., Drucker, S., Fernandez, R., and Heer, J. (2015). Refinery: Visual exploration of large, heterogeneous networks through associative browsing. In <i>Computer Graphics Forum</i> (Vol. 34, pp. 301â310).</div>",
        "tags": {
            "collaborators": [
                "Kairam",
                "Riche",
                "Fernandez",
                "Heer"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Network"
            ],
            "year": [
                "2015"
            ],
            "publication": [
                "EuroVis"
            ]
        },
        "abstract": "Browsing is a fundamental aspect of exploratory information-seeking. Associative browsing encompasses a common and intuitive set of exploratory strategies in which users step iteratively from familiar to novel pieces of information. In this paper, we consider associative browsing as a strategy for bottom-up exploration of large, heterogeneous networks. We present Refinery, an interactive visualization system informed by guidelines drawn from examination of several areas of literature related to exploratory information-seeking. These guidelines motivate Refinery???s query model, which allows users to simply and expressively construct queries using heterogeneous sets of nodes. The system ranks and returns associated content using a fast, random-walk based algorithm, visualizing results and connections among them to provide explanatory context, facilitate serendipitous discovery, and stimulate continued exploration. A study of 12 academic researchers using Refinery to browse publication data related to areas of study demonstrates how the system complements existing tools in supporting discovery.",
        "primary": "Visualization"
    },
    {
        "caption": "Creativity Support",
        "img": "researchImages/creativity.png",
        "bibEntry": "<div class=\"csl-entry\">Kerne, A., Webb, A. M., Latulipe, C., Carroll, E., Drucker, S. M., Candy, L., and HÃ¶Ã¶k, K. (2013). Evaluation methods for creativity support environments. In <i>CHIâ13 Extended Abstracts on Human Factors in Computing Systems</i> (pp. 3295â3298). ACM.</div>",
        "tags": {
            "collaborators": [
                "Kerne",
                "Webb",
                "Latulipe",
                "Carroll",
                "Candy"
            ],
            "subject": [
                "UI"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "primary": "UI-Information"
    },
    {
        "caption": "Completion",
        "img": "researchImages/completion.png",
        "bibEntry": "<div class=\"csl-entry\">Kopf, J., Kienzle, W., Drucker, S., and Kang, S. B. (2012). Quality prediction for image completion. <i>ACM Transactions on Graphics (TOG)</i>, <i>31</i>(6), 131.</div>",
        "tags": {
            "collaborators": [
                "Kopf",
                "Kienzle",
                "Kang"
            ],
            "subject": [
                "Graphics",
                "Photos"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "SIGGRAPHAsia"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/completion.pdf",
        "video": "http://research.microsoft.com/apps/video/default.aspx?id=173013",
        "abstract": "We present a data-driven method to predict the performance of an image completion method. Our image completion method is based on the state-of-the-art non-parametric framework of Wexler et al. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape before the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm.",
        "primary": "Photos"
    },
    {
        "caption": "Topic-Lead-Lag",
        "img": "researchImages/leadlag.png",
        "bibEntry": "<div class=\"csl-entry\">Liu, S., Chen, Y., Wei, H., Yang, J., Zhou, K., and Drucker, S. M. (2015). Exploring topical lead-lag across corpora. <i>IEEE Transactions on Knowledge and Data Engineering</i>, <i>27</i>(1), 115â129.</div>",
        "tags": {
            "collaborators": [
                "Liu",
                "Chen",
                "Wei",
                "Yang",
                "Zhou"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Machine Learning"
            ],
            "year": [
                "2014"
            ],
            "publication": [
                "TKDE"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/leadlag.pdf",
        "video": "",
        "abstract": "Identifying which text corpus leads in the context of a topic presents a great challenge of considerable interest to researchers. Recent research into lead-lag analysis has mainly focused on estimating the overall leads and lags between two corpora. However, real-world applications have a dire need to understand lead-lag patterns both globally and locally. In this paper, we introduce TextPioneer, an interactive visual analytics tool for investigating lead-lag across corpora from the global level to the local level. In particular, we extend an existing lead-lag analysis approach to derive two-level results. To convey multiple perspectives of the results, we have designed two visualizations, a novel hybrid tree visualization that couples a radial space-filling tree with a node-link diagram and a twisted-ladder-like visualization. We have applied our method to several corpora and the evaluation shows promise, especially in support of text comparison at different levels of detail.",
        "primary": "Visualization"
    },
    {
        "caption": "Annotation Gigapixel Images",
        "img": "researchImages/annotategigapixel.jpg",
        "bibEntry": "<div class=\"csl-entry\">Luan, Q., Drucker, S. M., Kopf, J., Xu, Y.-Q., and Cohen, M. F. (2008). Annotating gigapixel images. In <i>Proceedings of the 21st annual ACM symposium on User interface software and technology</i> (pp. 33â36). ACM.</div>",
        "tags": {
            "collaborators": [
                "Cohen",
                "Luan",
                "Kopf",
                "Xu"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Photos"
            ],
            "year": [
                "2008"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/uist2008annotating.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/Pixaura_CHI_08_v3.mov",
        "abstract": "Panning and zooming interfaces for exploring very large images containing billions of pixels (gigapixel images) have recently appeared on the internet. This paper addresses issues that arise when creating and rendering auditory and textual annotations for such images. In particular, we define a distance metric between each annotation and any view resulting from panning and zooming on the image. The distance then informs the rendering of audio annotations and text labels. We demonstrate the annotation system on a number of panoramic images.",
        "primary": "Photos"
    },
    {
        "caption": "Contextual Facets",
        "img": "researchImages/thumbnail_contextualfacets.png",
        "bibEntry": "<div class=\"csl-entry\">Medynskiy, Y., Dontcheva, M., and Drucker, S. M. (2009). Exploring websites through contextual facets. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 2013â2022). ACM.</div>",
        "tags": {
            "collaborators": [
                "Dontcheva",
                "Medynskiy"
            ],
            "subject": [
                "UI",
                "Information",
                "Search"
            ],
            "year": [
                "2009"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/ResearchContent/papers/2008CHI_Contextual_Facets.pdf",
        "abstract": "We present contextual facets, a novel user interface technique for navigating websites that publish large collections of semi-structured data. Contextual facets extend traditional faceted navigation techniques by transforming webpage elements into user interface components for filtering and retrieving related webpages. To investigate users' reactions to contextual facets, we built FacetPatch, a web browser that automatically generates contextual facet interfaces. As the user browses the web, FacetPatch automatically extracts semi-structured data from collections of webpages and overlays contextual facets on top of the current page. Participants in an exploratory user evaluation of FacetPatch were enthusiastic about contextual facets and often preferred them to an existing, familiar faceted navigation interface. We discuss how we improved the design of contextual facets and FacetPatch based on the results of this study.",
        "primary": "UI-Information"
    },
    {
        "caption": "Step User Interfaces",
        "img": "researchImages/stepUI27.jpg",
        "bibEntry": "<div class=\"csl-entry\">Meyers, B., Brush, A., Drucker, S., Smith, M. A., and Czerwinski, M. (2006). Dance your work away: exploring step user interfaces. In <i>CHIâ06 extended abstracts on Human factors in computing systems</i> (pp. 387â392). ACM.</div>",
        "tags": {
            "collaborators": [
                "Meyers",
                "Brush",
                "Smith",
                "Czerwinski"
            ],
            "subject": [
                "UI",
                "Photos"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/stepUICHI06.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/StepStar.wmv",
        "abstract": "While applications are typically optimized for traditional desktop interfaces using a keyboard and mouse, there are a variety of compelling reasons to consider alternative input mechanisms that require more physical exertion, including promoting fitness, preventing Repetitive Strain Injuries, and encouraging fun. We chose to explore physical interfaces based on foot motion and have built two applications with Step User Interfaces: StepMail and StepPhoto. Both support working with email and photos using the dance pad made popular by the Dance Dance Revolution (DDR) game. Results of a formative evaluation with ten participants suggest that the interactions are intuitive to learn, somewhat enjoyable, and cause participants to increase their level of exertion over sitting at a desk. Our evaluation also revealed design considerations for Step User Interfaces, including balancing effort across the body, avoiding needless exertion, and choosing target applications with care.",
        "primary": "Photos"
    },
    {
        "caption": "GestureElicitation",
        "img": "researchImages/gestureelicitation.png",
        "bibEntry": "<div class=\"csl-entry\">Morris, M. R., Danielescu, A., Drucker, S., Fisher, D., Lee, B., and Wobbrock, J. O. (2014). Reducing legacy bias in gesture elicitation studies. <i>Interactions</i>, <i>21</i>(3), 40â45.</div>",
        "tags": {
            "collaborators": [
                "Morris",
                "Danielescu",
                "Fisher",
                "Lee",
                "schraefel",
                "Wobbrock"
            ],
            "subject": [
                "UI"
            ],
            "year": [
                "2013"
            ],
            "publication": [
                "Interact"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/gesture_elicitation_interactions.pdf",
        "video": "",
        "abstract": "Improving methods for choosing appropriate gestures for novel user interaction techniques.",
        "primary": "UI-Information"
    },
    {
        "caption": "Gestalt",
        "img": "researchImages/gestalt.png",
        "bibEntry": "<div class=\"csl-entry\">Patel, K., Bancroft, N., Drucker, S. M., Fogarty, J., Ko, A. J., and Landay, J. (2010). Gestalt: integrated support for implementation and analysis in machine learning. In <i>Proceedings of the 23nd annual ACM symposium on User interface software and technology</i> (pp. 37â46). ACM.</div>",
        "tags": {
            "collaborators": [
                "Patel",
                "Bancroft",
                "Fogarty",
                "Ko",
                "Landay"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Machine Learning",
                "Programming"
            ],
            "year": [
                "2010"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/uist2010gestalt.pdf",
        "video": "http://www.youtube.com/watch?v=9XC-D2L93jA&feature=player_embedded",
        "abstract": "We present Gestalt, a development environment designed tosupport the process of applying machine learning. While traditional programming environments focus on source code, we explicitly support both code and data. Gestalt allows developers to implement a classification pipeline, analyze data as it moves through that pipeline, and easily transition between implementation and analysis. An experiment shows this significantly improves the ability of developers to find and fix bugs in machine learning systems. Our discussion of Gestalt and our experimental observations provide new insight into general-purpose support for the  achine learning process.",
        "primary": "Machine Learning"
    },
    {
        "caption": "Prospect",
        "img": "researchImages/prospect.png",
        "bibEntry": "<div class=\"csl-entry\">Patel, K., Drucker, S. M., Fogarty, J., Kapoor, A., and Tan, D. S. (2011). Using multiple models to understand data. In <i>IJCAI Proceedings-International Joint Conference on Artificial Intelligence</i> (Vol. 22, p. 1723).</div>",
        "tags": {
            "collaborators": [
                "Patel",
                "Kapoor",
                "Fogarty",
                "Ko",
                "Tan"
            ],
            "subject": [
                "UI",
                "Visualization",
                "Machine Learning"
            ],
            "year": [
                "2011"
            ],
            "publication": [
                "IJCAI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/ijcai11.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/visavis.wmv",
        "abstract": "A human's ability to diagnose errors, gather data, and generate features in order to build better models is largely untapped. We hypothesize that analyzing results from multiple models can help people diagnose errors by understanding relationships among data, features, and algorithms. These relationships might otherwise be masked by the bias inherent to any individual model. We demonstrate this approach in our Prospect system, show how multiple models can be used to detect label noise and aid in generating new features, and validate our methods in a pair of experiments.",
        "primary": "Machine Learning"
    },
    {
        "caption": "Sequence Preprocessing",
        "img": "researchImages/sequencepreprocessing.png",
        "bibEntry": "<div class=\"csl-entry\">Sarikaya, A., Zgraggen, E., DeLine, R., Drucker, S., and Fisher, D. (2016). Sequence Pre-processing: Focusing Analysis of Log Event Data.</div>",
        "tags": {
            "collaborators": [
                "Sarikaya",
                "Zgraggen",
                "DeLine",
                "Fisher"
            ],
            "subject": [
                "Information"
            ],
            "year": [
                "2016"
            ],
            "publication": [
                "Workshop"
            ]
        },
        "abstract": "Many computational systems are generating log event data as a way to help developers understand the usage of applications in the wild.  While many commercial analysis tools exist, they tend to treat log event data as a Ã¢bag of eventsÃ¢ instead of collections of observed sequences, where each sequence represents an individual session.  While recent work can support the visual analysis of event sequence data, log files tend to contain complexity in scale and noise that can foul downstream analyses.  In this work, we identify common recurring problems of noise that arise from the analysis of this data, and assert that methods for preprocessing can be a valuable tool to both focus data for downstream analysis and provide provenance support for visual analytics tools. These pre-processing methods can be performed interactively and in conjunction with analysis tools to iteratively refine rules to streamline visual analysis. Through several case studies, we identify the common sources of noise in log files and demonstrate how our proposed pre-processing methods can help to minimize excess data reaching downstream analysis tools. ",
        "primary": "UI-Information"
    },
    {
        "caption": "Pixaura",
        "img": "researchImages/pixAura.jpg",
        "bibEntry": "<div class=\"csl-entry\">Satpathy, L., Kamppari, S., Lewis, B., Prasad, A., Rhee, Y. W., Elgart, B., and Drucker, S. (2008). Pixaura: supporting tentative decision making when selecting and sharing digital photos. In <i>Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction-Volume 2</i> (pp. 87â91). British Computer Society.</div>",
        "tags": {
            "collaborators": [
                "Elgart",
                "Kamppari",
                "Lewis",
                "Prasad",
                "Rhee",
                "Satpathy"
            ],
            "subject": [
                "UI",
                "Photos"
            ],
            "year": [
                "2008"
            ],
            "publication": [
                "HCI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/HCI_2008_TentativeDecisionsPixaurafinal.pdf",
        "abstract": "Current advances in digital technology promote capturing and storing more digital photos than ever. While photo collections are growing in size, the amount of time that can be devoted to viewing, managing, and sharing digital photos remains constant. Photo decision-making and selection has been identified as key to addressing this concern. After conducting exploratory research on photo decision-making including a wide-scale survey of user behaviors, detailed contextual inquiries, and longer-term diary studies, Pixaura was designed to address problems that emerged from our research. Specifically, Pixaura aims to bridge the gap between importing source photos and sharing them with others, by supporting tentative decision-making within the selection process. For this experience, the system incorporates certain core elements: 1) flexibility to experiment with relationships between photos and groups of photos, 2) the ability to closely couple photos while sharing only a subset of those photos, and 3) a tight connection between the photo selection and photo sharing space.",
        "primary": "Photos"
    },
    {
        "caption": "Parallel Raytracing",
        "img": "researchImages/dataparallel.gif",
        "bibEntry": "<div class=\"csl-entry\">Schroder, P., and Drucker, S. (1992). A data parallel algorithm for raytracing of heterogeneous databases. <i>Proceedings of Computer Graphics Interface</i>, 167â175.</div>",
        "tags": {
            "collaborators": [
                "Schroeder"
            ],
            "subject": [
                "Graphics",
                "Parallel Computing"
            ],
            "year": [
                "1992"
            ],
            "publication": [
                "GI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/GIraytrace.pdf",
        "abstract": "We describe a new data parallel algorithm for raytracing. Load balancing is achieved through the use of processor allocation, which continually remaps available resources. In this manner heterogeneous data bases are handled without the usual problems of low resource usage. The proposed approach adapts well to both extremes: a small number of rays and a large database; a large number of rays and a small database. The algorithm scales linearly|over a wide range|in the number of rays and available processors. We present an implementation on the Connection Machine CM2 system and provide timings.",
        "primary": "Graphics"
    },
    {
        "caption": "Find That Photo",
        "img": "researchImages/findthatphoto.png",
        "bibEntry": "<div class=\"csl-entry\">Shneiderman, B., Bederson, B. B., and Drucker, S. M. (2006). Find that photo!: interface strategies to annotate, browse, and share. <i>Communications of the ACM</i>, <i>49</i>(4), 69â71.</div>",
        "tags": {
            "collaborators": [
                "Shneiderman",
                "Bederson"
            ],
            "subject": [
                "Photos"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                ""
            ]
        },
        "primary": "Photos"
    },
    {
        "caption": "Tactile Sensor",
        "img": "researchImages/perfanalysis.png",
        "bibEntry": "<div class=\"csl-entry\">Siegel, D. M., Drucker, S. M., and Garabieta, I. (1987). <i>Performance analysis of a tactile sensor</i>. IEEE.</div>",
        "tags": {
            "collaborators": [
                "Siegel",
                "Garabieta"
            ],
            "subject": [
                "Robotics",
                "Touch"
            ],
            "year": [
                "1987"
            ],
            "publication": [
                "IEEE"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/tactilesensor.pdf",
        "abstract": "This paper discusses the design of a contact sensor for use with the Utah-MIT dexterous hand [Jacobsen, et al. 1984]. The sensor utilizes an 8x8 array of capacitive cells. This paper extends the work presented in Siegel and Garabiet [1986], and the ealier work of Boie [1984]; a more detailed design analysis, modifications to the construction process, and better performance results are shown.",
        "primary": "Robotics"
    },
    {
        "caption": "Counting Community",
        "img": "researchImages/countingcommunities.png",
        "bibEntry": "<div class=\"csl-entry\">Smith, M. A., Drucker, S. M., Kraut, R., and Wellman, B. (1999). Counting on community in cyberspace. In <i>CHIâ99 Extended Abstracts on Human Factors in Computing Systems</i> (pp. 87â88). ACM.</div>",
        "tags": {
            "collaborators": [
                "Smith",
                "Kraut",
                "Wellman"
            ],
            "subject": [
                "Social"
            ],
            "year": [
                "1999"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "primary": "Social"
    },
    {
        "caption": "Social Life of Avatars",
        "img": "researchImages/socialavatar.jpg",
        "bibEntry": "<div class=\"csl-entry\">Smith, M. A., Farnham, S. D., and Drucker, S. M. (2000). The social life of small graphical chat spaces. In <i>Proceedings of the SIGCHI conference on Human Factors in Computing Systems</i> (pp. 462â469). ACM.</div>",
        "tags": {
            "collaborators": [
                "Smith",
                "Farnham"
            ],
            "subject": [
                "Social"
            ],
            "year": [
                "2000"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/chisoclife.pdf",
        "abstract": "This paper provides a unique quantitative analysis of the social dynamics of three chat rooms in the Microsoft V-Chat graphical chat system. Survey and behavioral data were used to study user experience and activity. 150 V-Chat participants completed a web-based survey, and data logs were collected from three V-Chat rooms over the course of 119 days. This data illustrates the usage patterns of graphical chat systems, and highlights the ways physical proxemics are translated into social interactions in online Environments. V-Chat participants actively used gestures, avatars, and movement as part of their social interactions. Analyses of clustering patterns and movement data show that avatars were used to provide nonverbal cues similar to those found in face-to-face interactions. However, use of some graphical features, in particular gestures, declined as users became more experienced with the system. These findings have implications for the design and study of online interactive environments.",
        "primary": "Social"
    },
    {
        "caption": "Visual Snippets",
        "img": "researchImages/thumbnail_visualsnippets.png",
        "bibEntry": "<div class=\"csl-entry\">Teevan, J., Cutrell, E., Fisher, D., Drucker, S. M., Ramos, G., AndrÃ©, P., and Hu, C. (2009). Visual snippets: summarizing web pages for search and revisitation. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 2023â2032). ACM.</div>",
        "tags": {
            "collaborators": [
                "Teevan",
                "Cutrell",
                "Fisher",
                "Ramos",
                "Andre",
                "Hu"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Design",
                "Search"
            ],
            "year": [
                "2009"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/2008CHI_VisualSnippets.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/facetPatchCameraReady.mov",
        "abstract": "People regularly interact with different representations of Web pages. A person looking for new information may initially find a Web page represented as a short snippet rendered by a search engine. When he wants to return to the same page the next day, the page may instead be represented by a link in his browser history. Previous research has explored how to best represent Web pages in support of specific task types, but, as we find in this paper, consistency in representation across tasks is also important. We explore how different representations are used in a variety of contexts and present a compact representation that supports both the identification of new, relevant Web pages and the re-finding of previously viewed pages.",
        "primary": "Visualization"
    },
    {
        "caption": "reForm",
        "img": "researchImages/thumbnail_reform.jpg",
        "bibEntry": "<div class=\"csl-entry\">Toomim, M., Drucker, S. M., Dontcheva, M., Rahimi, A., Thomson, B., and Landay, J. A. (2009). Attaching UI enhancements to websites with end users. In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i> (pp. 1859â1868). ACM.</div>",
        "tags": {
            "collaborators": [
                "Toomim",
                "Dontcheva",
                "Rahimi",
                "Thomson",
                "Landay"
            ],
            "subject": [
                "UI",
                "Information",
                "Web"
            ],
            "year": [
                "2009"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/2008CHI_reform.pdf",
        "video": "http://research.microsoft.com/~sdrucker/video/ThumbtackFinal/ThumbtackIntroductionVideo.wmv",
        "abstract": "There are not enough programmers to support all end user goals by building websites, mashups, and browser extensions. This paper presents reform, a system that envisions roles for both programmers and end-users in creating enhancements of existing websites that support new goals. Programmers author a traditional mashup or browser extension, but instead of writing a web scraper by hand, the reform system enables novice end users to attach the mashup to their websites of interest. reform both makes scraping easier for the programmer and carries the benefit that endusers can retarget the enhancements towards completely different web sites, using a new programming by example interface and machine learning algorithm for web data extraction. This work presents reform's architecture, algorithms, user interface, evaluation, and five example reform enabled enhancements that provide a step towards our goal of write-once apply-anywhere user interface enhancements.",
        "primary": "UI-Information"
    },
    {
        "caption": "VWorlds",
        "img": "researchImages/vworlds.png",
        "bibEntry": "<div class=\"csl-entry\">Vellon, M., Marple, K., Mitchell, D., and Drucker, S. (1998). The Architecture of a Distributed Virtual Worlds System. In <i>COOTS</i> (pp. 211â218).</div>",
        "tags": {
            "collaborators": [
                "Vellon",
                "Marple",
                "Mitchell"
            ],
            "subject": [
                "Graphics",
                "UI",
                "Social"
            ],
            "year": [
                "1998"
            ],
            "publication": [
                "OOPSLA"
            ]
        },
        "primary": "Graphics"
    },
    {
        "caption": "Streaming Chat",
        "img": "researchImages/streamChat.jpg",
        "bibEntry": "<div class=\"csl-entry\">Vronay, D., Smith, M., and Drucker, S. (1999). Alternative interfaces for chat. In <i>Proceedings of the 12th annual ACM symposium on User interface software and technology</i> (pp. 19â26). ACM.</div>",
        "tags": {
            "collaborators": [
                "Vronay",
                "Smith"
            ],
            "subject": [
                "UI",
                "Social"
            ],
            "year": [
                "1999"
            ],
            "publication": [
                "UIST"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/chat.pdf",
        "abstract": "We describe some common problems experienced by users of computer-based text chat, and show how many of these problems relate to the loss of timing-specific information.  We suggest that thinking of chat as a streaming media data type might solve some of these problems.  We then present a number of alternative chat interfaces along with results from user studies comparing and contrasting them both with each other and with the standard chat interface.",
        "primary": "Social"
    },
    {
        "caption": "Cartoon Animation Filter",
        "img": "researchImages/aniequation.jpg",
        "bibEntry": "<div class=\"csl-entry\">Wang, J., Drucker, S. M., Agrawala, M., and Cohen, M. F. (2006). The cartoon animation filter. In <i>ACM Transactions on Graphics (TOG)</i> (Vol. 25, pp. 1169â1173). ACM.</div>",
        "tags": {
            "collaborators": [
                "Wang",
                "Agrawala",
                "Cohen"
            ],
            "subject": [
                "Graphics",
                "Animation"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "SIGGRAPH"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/TheCartoonAnimationFilter.pdf",
        "video": "http://research.microsoft.com/~sdrucker/Video/af.mov",
        "abstract": "We present the 'Cartoon Animation Filter', a simple filter that takes an arbitrary input motion signal and modulates it in such a way that the output motion is more 'alive' or 'animated'. The filter adds a smoothed, inverted, and (sometimes) time shifted version of the second derivative (the acceleration) of the signal back into the original signal. Almost all parameters of the filter are automated. The user only needs to set the desired strength of the filter. The beauty of the animation filter lies in its simplicity and generality. We apply the filter to motions ranging from hand drawn trajectories, to simple animations within PowerPoint presentations, to motion captured DOF curves, to video segmentation results. Experimental results show that the filtered motion exhibits anticipation, follow-through, exaggeration and squash-and-stretch effects which are not present in the original input motion data.",
        "primary": "Graphics"
    },
    {
        "caption": "Web Search Variability",
        "img": "researchImages/websearch.png",
        "bibEntry": "<div class=\"csl-entry\">White, R. W., and Drucker, S. M. (2007). Investigating behavioral variability in web search. In <i>Proceedings of the 16th international conference on World Wide Web</i> (pp. 21â30). ACM.</div>",
        "tags": {
            "collaborators": [
                "White"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Search",
                "Web"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "WWW"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/WhiteWWW2007.pdf",
        "abstract": "Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people's interaction behavior when engaged in search-related activities on the Web. We analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles. The findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit. Our findings also suggest two classes of extreme user navigators and explorers whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone.",
        "primary": "UI-Information"
    },
    {
        "caption": "Search Workshop",
        "img": "researchImages/expsearchint.png",
        "bibEntry": "<div class=\"csl-entry\">White, R. W., Drucker, S. M., Marchionini, G., Hearst, M., and others. (2007). Exploratory search and HCI: designing and evaluating interfaces to support exploratory search interaction. In <i>CHIâ07 Extended Abstracts on Human Factors in Computing Systems</i> (pp. 2877â2880). ACM.</div>",
        "tags": {
            "collaborators": [
                "White",
                "Marchionini",
                "Hearst",
                "schraefel"
            ],
            "subject": [
                "Visualization",
                "UI",
                "Search",
                "Web"
            ],
            "year": [
                "2007"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/p2877-white.pdf",
        "video": "",
        "abstract": "The model of search as a turn-taking dialogue between the user and an intermediary has remained unchanged for decades. However, there is growing interest within the search community in evolving this model to support search-driven information exploration activities. So-called exploratory search describes a class of search activities that move beyond fact retrieval toward fostering learning, investigation, and information use. Exploratory search interaction focuses on the user-system communication essential during exploratory search processes. Given this user-centered focus, the CHI conference is an ideal venue to discuss mechanisms to support exploratory searchbehaviors. Specifically, this workshop aims to gather researchers, academics, and practitioners working in human-computer interaction, information retrieval, and other related disciplines, for a discussion of the issues relating to the design and evaluation of interfaces to help users explore, learn, and use information. These are important issues with far-reaching implications for how many computer users accomplish their tasks.",
        "primary": "UI-Information"
    },
    {
        "caption": "Exploratory Search",
        "img": "researchImages/CACM.png",
        "bibEntry": "<div class=\"csl-entry\">White, R. W., Kules, B., Drucker, S. M., and others. (2006). Supporting exploratory search, introduction, special issue, communications of the ACM. <i>Communications of the ACM</i>, <i>49</i>(4), 36â39.</div>",
        "tags": {
            "collaborators": [
                "White",
                "Kules",
                "schraefel"
            ],
            "subject": [
                "UI",
                "Search"
            ],
            "year": [
                "2006"
            ],
            "publication": [
                "ACM"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/2007introacm.pdf",
        "primary": "UI-Information"
    },
    {
        "caption": "Visualization Surfaces",
        "img": "researchImages/visualizationsurfaces.png",
        "bibEntry": "<div class=\"csl-entry\">Whitted, T., and Drucker, S. (2012). Visualization Surfaces. In <i>Expanding the Frontiers of Visual Analytics and Visualization</i> (pp. 417â427). Springer London.</div>",
        "tags": {
            "collaborators": [
                "Whitted"
            ],
            "subject": [
                "UI",
                "Visualization"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "Book"
            ]
        },
        "abstract": "Large displays suitable for visualization applications are typically constructed from arrays of smaller ones. The physical and optical challenges of designing these assemblies are the first topic addressed here.",
        "primary": "Visualization"
    },
    {
        "caption": "Collaborative Visualization",
        "img": "researchImages/collab.jpg",
        "bibEntry": "<div class=\"csl-entry\">Xiong, R., Smith, M. A., and Drucker, S. M. (1998). Visualizations of collaborative information for end-users. <i>Microsoft Research</i>, 1â8.</div>",
        "tags": {
            "collaborators": [
                "Xiong",
                "Smith"
            ],
            "subject": [
                "Visualization",
                "Social"
            ],
            "year": [
                "1999"
            ],
            "publication": [
                "InternalReport"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/collabviz.pdf",
        "abstract": "There is a growing need for methods and tools to illuminate the social contexts of interaction environments created by the World Wide Web, Usenet newsgroups, email lists, and other network interaction media. We present here a framework for creating visualizations of the social connections created in and through network interaction media. Using graph-drawing methods, visualizations can be created for a range of systems that link people to people and people to objects through networks. As an example, we present an application of our methods to the Usenet to illustrate how visualization can improve existing systems.  We propose that users of network interaction media can benefit from visualizations that illuminate the interaction context generated by the rich interconnections between groups, conversations, and people in these media.",
        "primary": "Social"
    },
    {
        "caption": "Intermedia2",
        "img": "researchImages/intermedia2.gif",
        "bibEntry": "<div class=\"csl-entry\">Yankelovich, N., Haan, B., and Drucker, S. (1987). Connections in Context: The intermedia system. <i>Providence, Rhode Island: Institute for Research in Information and Scholarship, Brown University</i>.</div>",
        "tags": {
            "collaborators": [
                "Yankelovich",
                "Haan"
            ],
            "subject": [
                "Hyptertext",
                "UI",
                "Information"
            ],
            "year": [
                "1988"
            ],
            "publication": [
                "ICSS"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/connintermedia.pdf",
        "abstract": "none",
        "primary": "Hypertext"
    },
    {
        "caption": "Intermedia1",
        "img": "researchImages/intermedia.gif",
        "bibEntry": "<div class=\"csl-entry\">Yankelovich, N., Haan, B. J., Meyrowitz, N. K., and Drucker, S. M. (1988). Intermedia: The concept and the construction of a seamless information environment. <i>IEEE Computer</i>, <i>21</i>(1), 81â96.</div>",
        "tags": {
            "collaborators": [
                "Yankelovich",
                "Haan",
                "Meyrowitz"
            ],
            "subject": [
                "Hyptertext",
                "UI",
                "Information"
            ],
            "year": [
                "1988"
            ],
            "publication": [
                "IEEE"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/intermedia1.pdf",
        "abstract": "none",
        "primary": "Hypertext"
    },
    {
        "caption": "Mission Planner",
        "img": "researchImages/darpa.gif",
        "bibEntry": "<div class=\"csl-entry\">Zeltzer, D., and Drucker, S. (1992). A virtual environment system for mission planning. In <i>Proceedings of the IMAGE VI conference</i> (pp. 125â134).</div>",
        "tags": {
            "collaborators": [
                "Zeltzer"
            ],
            "subject": [
                "Graphics",
                "Camera",
                "Thesis"
            ],
            "year": [
                "1994"
            ],
            "publication": [
                "Thesis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/mission.pdf",
        "abstract": "A key function of a mission planning system is to enhance and maintain situational awareness of planning personnel and aircrews who will use the system for pre-mission rehearsals and briefings. We have developed a mission planner using virtual environment technology. We provide a task level interface to computational models of aircraft, terrain, threats and targets, so that users interact directly with these models using voice and gesture recognition, 3D positional input, 3 axis force output, and intelligent camera control.",
        "primary": "Camera"
    },
    {
        "caption": "Squeries",
        "img": "researchImages/squeries.png",
        "bibEntry": "<div class=\"csl-entry\">Zgraggen, E., Drucker, S., Fisher, D., and DeLine, R. (2015). <i>(s|qu)eries: Visual Regular Expressions for Querying and Exploring Event Sequences</i>. ACM Association for Computing Machinery.</div>",
        "tags": {
            "collaborators": [
                "Zgraggen",
                "Fisher",
                "DeLine"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Touch",
                "Sequences"
            ],
            "year": [
                "2015"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/chi2015-squeries.pdf",
        "abstract": "Many different domains collect event sequence data and rely on finding and analyzing patterns within it to gain meaningful insights. Current systems that support such queries either provide limited expressiveness, hinder exploratory workflows or present interaction and visualization models which do not scale well to large and multi-faceted data sets. In this paper we present (s|qu)eries (pronounced 'Squeries'), a visual query interface for creating queries on sequences (series) of data, based on regular expressions. (s|qu)eries is a touchbased system that exposes the full expressive power of regular expressions in an approachable way and interleaves query specification with result visualizations. Being able to visually investigate the results of different query-parts supports debugging and encourages iterative query-building as well as exploratory work-flows. We validate our design and implementation through a set of informal interviews with data scientists that analyze event sequences on a daily basis.",
        "primary": "Visualization"
    },
    {
        "caption": "Panoramic Data",
        "img": "researchImages/panodata.png",
        "bibEntry": "<div class=\"csl-entry\">Zgraggen, E., Zeleznik, R., and Drucker, S. M. (2014). PanoramicData: Data analysis through pen and touch. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>20</i>(12), 2112â2121.</div>",
        "tags": {
            "collaborators": [
                "Zgraggen",
                "Zeleznik"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Touch"
            ],
            "year": [
                "2014"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/PanoramicData.pdf",
        "video": "",
        "abstract": "Interactively exploring multidimensional datasets requires frequent switching among a range of distinct but inter-related tasks (e.g., producing different visuals based on different column sets, calculating new variables, and observing the interactions between sets of data). Existing approaches either target specific different problem domains (e.g., data-transformation or datapresentation) or expose only limited aspects of the general exploratory process; in either case, users are forced to adopt coping strategies (e.g., arranging windows or using undo as a mechanism for comparison instead of using side-by-side displays) to compensate for the lack of an integrated suite of exploratory tools. PanoramicData (PD) addresses these problems by unifying a comprehensive set of tools for visual data exploration into a hybrid pen and touch system designed to exploit the visualization advantages of large interactive displays. PD goes beyond just familiar visualizations by including direct UI support for data transformation and aggregation, filtering and brushing. Leveraging an unbounded whiteboard metaphor, users can combine these tools like building blocks to create detailed interactive visual display networks in which each visualization can act as a filter for others. Further, by operating directly on relational-databases, PD provides an approachable visual language that exposes a broad set of the expressive power of SQL, including functionally complete logic filtering, computation of aggregates and natural table joins. To understand the implications of this novel approach, we conducted a formative user study with both data and visualization experts. The results indicated that the system provided a fluid and natural user experience for probing multi-dimensional data and was able to cover the full range of queries that the users wanted to pose",
        "primary": "Visualization"
    },
    {
        "caption": "Immersive VA Interaction",
        "img": "researchImages/immersiveinteraction.png",
        "bibEntry": "<div class=\"csl-entry\">Buschell, W., Chen, J., Dachselt, R., Drucker, S., Dwyer, T., Gorg, C., … Stuerzlinger, W. (2018). Interaction for Immersive Analytics. In <i>Immersive Analytics</i> (pp. 95–138).</div>",
        "tags": {
            "collaborators": [
                "Buschell",
                "Chen",
                "Dachselt",
                "Dwyer",
                "Gorg",
                "Isenberg",
                "Kerren",
                "North",
                "Stuerzlinger"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Graphics"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "Book"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/immersiveinteraction.pdf",
        "abstract": "In this chapter, we briefly review the development of natural user interfaces and discuss their role in providing human-computer interaction that is immersive in various ways. Then we examine some opportunities for how these technologies might be used to better support data analysis tasks. Specifically, we review and suggest some interaction design guidelines for immersive analytics. We also review some hardware setups for data visualization that are already archetypal. Finally, we look at some emerging system designs that suggest future directions",
        "primary": "Visualization"
    },
    {
        "caption": "TimeSlice",
        "img": "researchImages/timeslice.png",
        "bibEntry": "<div class=\"csl-entry\">Zhao, J., Drucker, S. M., Fisher, D., and Brinkman, D. (2012). TimeSlice: Interactive faceted browsing of timeline data. In <i>Proceedings of the International Working Conference on Advanced Visual Interfaces</i> (pp. 433â436). ACM.</div>",
        "tags": {
            "collaborators": [
                "Zhao",
                "Fisher",
                "Brinkman"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "Temporal"
            ],
            "year": [
                "2012"
            ],
            "publication": [
                "AVI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/timeslice.pdf",
        "abstract": "Temporal events with multiple sets of metadata attributes, i.e., facets, are ubiquitous across different domains. The capabilities of efficiently viewing and comparing events data from various perspectives are critical for revealing relationships, making hypotheses, and discovering patterns. In this paper, we present TimeSlice, an interactive faceted visualization of temporal events, which allows users to easily compare and explore timelines with different attributes on a set of facets. By directly manipulating the filtering tree, a dynamic visual representation of queries and filters in the facet space, users can simultaneously browse the focused timelines and their contexts at different levels of detail, which supports efficient navigation of multi-dimensional events data. Also presented is an initial evaluation of TimeSlice with two datasets - famous deceased people and US daily flight delays.",
        "primary": "Visualization"
    },
    {
        "caption": "AnchorViz",
        "img": "researchImages/anchorviz.png",
        "bibEntry": "<div class=\"csl-entry\">Suh, J., Ghorashi, S, Ramos, G., Chen, N., Drucker, S., Verwey, J. & Simard, P. (2019). AnchorViz: Facilitating Semantic Data Exploration andConcept Discovery for Interactive Machine Learning. In <i>Transactions on Interactive Intelligent Systems</i> ACM. https://doi.org/10.1145/3172944.3172950</div>",
        "tags": {
            "collaborators": [
                "Suh",
                "Ghorashi",
                "Ramos",
                "Chen",
                "Verwey",
                "Simard"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "TIIS"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/AnchorViz.pdf",
        "abstract": "When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items.         The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness).  Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents \textit{AnchorViz}, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration.         By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts, and (b) surfaces the prediction and label inconsistencies between data points that are semantically related.         Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop.        We evaluated AnchorViz through two user studies.         Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods.         Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features.         We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.",
        "primary": "Visualization"
    },
    {
        "caption": "ATOM",
        "img": "researchImages/atom.png",
        "bibEntry": "<div class=\"csl-entry\">Park, D., Drucker, S. M., Fernandez, R., &amp; Elmqvist, N. (2018). Atom: A Grammar for Unit Visualizations. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>24</i>(12), 3032–3043. https://doi.org/10.1109/TVCG.2017.2785807</div>",
        "tags": {
            "collaborators": [
                "Park",
                "Fernandez",
                "Elmqvist"
            ],
            "subject": [
                "Information",
                "Visualization"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "TVCG"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/atom.pdf",
        "abstract": "Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark-a visual unit-during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called Atom, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.",
        "primary": "Visualization"
    },
    {
        "caption": "Barchart Differences",
        "img": "researchImages/whatsthediff.png",
        "bibEntry": "<div class=\"csl-entry\">Srinivasan, A., Brehmer, M., Lee, B., &amp; Drucker, S. M. (2018). What’s the Difference?: Evaluating Variations of Multi-Series Bar Charts for Visual Comparison Tasks. In <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i> (p. 304:1–304:12). New York, NY, USA: ACM. https://doi.org/10.1145/3173574.3173878</div>",
        "tags": {
            "collaborators": [
                "Srinivasn",
                "Brehmer",
                "Lee"
            ],
            "subject": [
                "Information",
                "Visualization"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "SIGCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/whatsthediff.pdf",
        "abstract": "An increasingly common approach to data analysis involves using information dashboards to visually compare changing data. However, layout constraints coupled with varying levels of visualization literacy among dashboard users make facilitating visual comparison in dashboards a challenging task. In this paper, we evaluate variants of bar charts, one of the most prevalent class of charts used in dashboards. We report an online experiment (N = 74) conducted to evaluate four alternative designs: 1) grouped bar chart, 2) grouped bar chart with difference overlays, 3) bar chart with difference overlays, and 4) difference bar chart. Results show that charts with difference overlays facilitate a wider range of comparison tasks while performing comparably to charts without them on individual tasks. Finally, we discuss the implications of our findings, with a focus on supporting visual comparison in dashboards.",
        "primary": "Visualization"
    },
    {
        "caption": "DNN Querying Videos",
        "img": "researchImages/QueryingVideos.png",
        "bibEntry": "<div class=\"csl-entry\">Wu, Y., Drucker, S., Philipose, M., &amp; Ravindranath, L. (2018). Querying Videos Using DNN Generated Labels. In <i>Proceedings of the Workshop on Human-In-the-Loop Data Analytics</i> (p. 6:1–6:6). New York, NY, USA: ACM. https://doi.org/10.1145/3209900.3209909</div>",
        "tags": {
            "collaborators": [
                "Wu",
                "Philipose",
                "Ravindranath"
            ],
            "subject": [
                "UI",
                "Information",
                "Media"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "HILDA"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/Querying.pdf",
        "abstract": "Massive amounts of videos are generated for entertainment, security, and science, powered by a growing supply of user-produced video hosting services. Unfortunately, searching for videos is difficult due to the lack of content annotations. Recent breakthroughs in image labeling with deep neural networks (DNNs) create a unique opportunity to address this problem. While many automated end-to-end solutions have been developed, such as natural language queries, we take on a different perspective: to leverage both the development of algorithms and human capabilities. To this end, we design a query language in tandem with a user interface to help users quickly identify segments of interest from the video based on labels and corresponding bounding boxes. We combine techniques from the database and information visualization communities to help the user make sense of the object labels in spite of errors and inconsistencies.",
        "primary": "Visualization"
    },
    {
        "caption": "FiberClay",
        "img": "researchImages/fiberclay.png",
        "bibEntry": " <div class=\"csl-entry\">Hurter, C., Riche, N. H., Drucker, S. M., Cordeil, M., Alligier, R., &amp; Vuillemot, R. (2019). FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>25</i>(1), 704–714. https://doi.org/10.1109/TVCG.2018.2865191</div>",
        "tags": {
            "collaborators": [
                "Hurter",
                "Riche",
                "Cordell",
                "Alligier",
                "Vuillemot"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "3D"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "TVCG"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/fiberclay.pdf",
        "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.",
        "primary": "Visualization"
    },
    {
        "caption": "Voder",
        "img": "researchImages/voder.png",
        "bibEntry": "<div class=\"csl-entry\">Srinivasan, A., Drucker, S. M., Endert, A., &amp; Stasko, J. (2019). Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication. <i>IEEE Transactions on Visualization and Computer Graphics</i>, <i>25</i>(1), 672–681. https://doi.org/10.1109/TVCG.2018.2865145</div>",
        "tags": {
            "collaborators": [
                "Srinivasn",
                "Endert",
                "Stasko"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "TVCG"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/Voder.pdf",
        "abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We presentVoder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.",
        "primary": "Visualization"
    },
    {
        "caption": "Immersive VAST",
        "img": "researchImages/immersivehuman.png",
        "bibEntry": "<div class=\"csl-entry\">Stuerzlinger, W., Dwyer, T., Drucker, S., Gorg, C., North, C., &amp; Scheuermann, G. (2019). Immersive human-centered computational analytics. In <i>Immersive Analytics</i> (pp. 139–163).</div>",
        "tags": {
            "collaborators": [
                "Stuerzlinger",
                "Dwyer",
                "Gorg",
                "North",
                "Scheuermann"
            ],
            "subject": [
                "UI",
                "Information",
                "Visualization",
                "3D"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "Book"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/immersivehuman.pdf",
        "abstract": "In this chapter we seek to elevate the role of the human in human-machine cooperative analysis through a careful consideration of immersive design principles. We consider both strategic immersion through more accessible systems as well as enhanced understanding and control through immersive interfaces that enable rapid workflows. We extend the classic sensemaking loop from visual analytics to incorporate multiple views, scenarios, people, and computational agents. We consider both sides of machine/human collaboration: allowing the human to more fluidly control the machine process; and also allowing the human to understand the results, derive insights and continue the analytic cycle. We also consider system and algorithmic implications of enabling real-time control and feedback in immersive human-centered computational analytics.",
        "primary": "Visualization"
    },
    {
        "caption": "Narrative Patterns",
        "img": "researchImages/narrativepatterns.png",
        "bibEntry": "<div class=\"csl-entry\">Bach, B., Stefaner, D., Boy, J., Drucker, S., Bartram, L., Wood, J., … Tversky, B. (2015). Narrative Design Patterns for Data-Driven Storytelling. <i>Data-Driven Storytelling</i>, 107–133. https://doi.org/10.1201/9781315281575-5</div>",
        "tags": {
            "collaborators": [
                "Bach",
                "Stefaner",
                "Boy",
                "Bartram",
                "Wood",
                "Ciuccarelli",
                "Engelhardt",
                "Koppen",
                "Tversky"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Communication"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "Book"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/narrativepatterns.pdf",
        "abstract": "This chapter introduces the concept of narrative design patterns, that aim to facilitate the shaping of compelling data-driven stories. There are many different ways storytellers can narrate the same story, depending on their intentions and their audience. Here, the authors define and describe a set of these narrative design patterns that can be used on their own or in combination to tell data stories in a myriad of ways. The authors then analyze 18 of them, and illustrate how these patterns can help storytellers think about the stories they want to tell and the best ways to narrate them. Each pattern has a specific purpose, for example, engaging the audience, evoking empathy, or creating flow and rhythm in the story. The authors assume storytellers already know what story they want to tell, why they want to tell it, and who they want to tell it to. These patterns may not only facilitate the process of creating compelling narratives, but stimulate a wider discussion on techniques and practices for data-driven storytelling.",
        "primary": "Visualization"
    },
    {
        "caption": "Data Audiences",
        "img": "researchImages/dataaudience.png",
        "bibEntry": "<div class=\"csl-entry\">Drucker, S., Huron, S., Kosara, R., Schwabish, J., Diakopolous, N. (2018). Communicating Data to an Audience. <i>Data-Driven Storytelling</i>, 229-250. https://doi.org/10.1201/9781315281575-5</div>",
        "tags": {
            "collaborators": [
                "Huron",
                "Kosara",
                "Schwabish",
                "Diakopolous"
            ],
            "subject": [
                "Information",
                "Visualization",
                "Communication"
            ],
            "year": [
                "2018"
            ],
            "publication": [
                "Book"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/dataaudience.pdf",
        "abstract": "Communicating data in an effective and efficient story requires the content author to recognize the needs, goals, and knowledge of the intended audience. To effectively communicate ideas and concepts, content authors need to think carefully about how their work best fits the needs of the audience. Data storytellers have an ethical obligation to ensure that their visualizations do not skew or mislead interpretations unnecessarily and this intersects with the literacy level of the expected audience. An individual in the audience brings their own viewpoints, backgrounds, and experience to each and every data-driven story they consume. Whether driven by their cultural background, social position, education, or other demographic characteristics, readers carry with them their own unique set of knowledge and biases. A consistent design approach can help an audience relate to a producer's work and entice them to return. Television audiences are also fickle, often switching programs and channels within less than a second.",
        "primary": "Visualization"
    },
    {
        "caption": "GAMut",
        "img": "researchImages/GAMut.PNG",
        "bibEntry": "<div class=\"csl-entry\">Fred Hohman, Andrew Head, Rich Caruana, Robert DeLine, and Steven M. Drucker. 2019. Gamut: A Design Probe to Understand How Data Scientists Understand Machine Learning Models. In <i>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19)</i>. ACM, New York, NY, USA, Paper 579, 13 pages. DOI: https://doi.org/10.1145/3290605.3300809</div>",
        "tags": {
            "collaborators": [
                "Hohman",
                "Head",
                "DeLine"
            ],
            "subject": [
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/19_gamut_chi.pdf",
        "abstract": "Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools.  Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, GAMut, to explore how interactive interfaces could better support model interpretation. Using GAMut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use GAMut in their work, highlighting its potential to help data scientists understand their own data.  ",
        "primary": "Visualization"
    },
    {
        "caption": "Semantic AnchorViz",
        "img": "researchImages/anchorvizextended.png",
        "bibEntry": "<div class=\"csl-entry\">Jina Suh Soroush Ghorashi Gonzalo Ramos Nan-Chen Chen Steven Drucker Johan Verwey Patrice Simard. (2019). AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning. <i>ACM Transactions on Interactive Intelligent Systems (TiiS) </i>. August 2019, Vol 10(1)</div>",
        "tags": {
            "collaborators": [
                "Suh",
                "Ghorash",
                "Ramos",
                "Chen",
                "Verwey",
                "Simard"
            ],
            "subject": [
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "TiiS"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/Tiis-AnchorViz-Suh.pdf",
        "abstract": "When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.",
        "primary": "Visualization"
    },
    {
        "caption": "TensorWatch",
        "img": "researchImages/TensorWatch.PNG",
        "bibEntry": "<div class=\"csl-entry\">Shital Shah Roland Fernandez Steven Drucker. (2019). A system for real-time interactive analysis of deep learning training. <i>Engineering Interactive Computing Systems (EICS) </i>.</div>",
        "tags": {
            "collaborators": [
                "Shah",
                "Fernandez"
            ],
            "subject": [
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "EICS"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/a16-shah.pdf",
        "abstract": "Performing diagnosis or exploratory analysis during the training of deep learning models is challenging but often necessary for making a sequence of decisions guided by the incremental observations. Currently available systems for this purpose are limited to monitoring only the logged data that must be specified before the training process starts. Each time a new information is desired, a cycle of stop-change-restart is required in the training process. These limitations make interactive exploration and diagnosis tasks difficult, imposing long tedious iterations during the model development. We present a new system that enables users to perform interactive queries on live processes generating real-time information that can be rendered in multiple formats on multiple surfaces in the form of several desired visualizations simultaneously. To achieve this, we model various exploratory inspection and diagnostic tasks for deep learning training processes as specifications for streams using a map-reduce paradigm with which many data scientists are already familiar. Our design achieves generality and extensibility by defining composable primitives which is a fundamentally different approach than is used by currently available systems",
        "primary": "Visualization"
    },
    {
        "caption": "Affinity Lens",
        "img": "researchImages/affinitylens.PNG",
        "bibEntry": "<div class=\"csl-entry\">Hariharan Subramonyam, Steven M. Drucker, and Eytan Adar. 2019. Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality. In <i>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19)</i>. ACM, New York, NY, USA, Paper 398, 13 pages. DOI: https://doi.org/10.1145/3290605.3300628</div>",
        "tags": {
            "collaborators": [
                "Subramonyam",
                "Adar"
            ],
            "subject": [
                "Information",
                "UI"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/AffinityLens_CHI_19-2.pdf",
        "abstract": "Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering unstructured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose Affinity Lens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative ‘views’ of data, without surrendering the lightweight benefits of existing AD practice.",
        "primary": "UI-Information"
    },
    {
        "caption": "Gather",
        "img": "researchImages/Gather.PNG",
        "bibEntry": "<div class=\"csl-entry\">Andrew Head, Fred Hohman, Titus Barik, Steven M. Drucker, and Robert DeLine. 2019. Managing Messes in Computational Notebooks. In <i>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19)</i>. ACM, New York, NY, USA, Paper 270, 12 pages. DOI: https://doi.org/10.1145/3290605.3300500</div>",
        "tags": {
            "collaborators": [
                "Head",
                "DeLine",
                "Hohman"
            ],
            "subject": [
                "Information",
                "Data"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/managingmesses.pdf",
        "abstract": "Data analysts use computational notebooks to write code for analyzing and visualizing data. Notebooks help analysts iteratively write analysis code by letting them interleave code with output, and selectively execute cells. However, as analysis progresses, analysts leave behind old code and outputs, and overwrite important code, producing cluttered and inconsistent notebooks. This paper introduces code gathering tools, extensions to computational notebooks that help analysts find, clean, recover, and compare versions of code in cluttered, inconsistent notebooks. The tools archive all versions of code outputs, allowing analysts to review these versions and recover the subsets of code that produced them. These subsets can serve as succinct summaries of analysis activity or starting points for new analyses. In a qualitative usability study, 12 professional analysts found the tools useful for cleaning notebooks and writing analysis code, and discovered new ways to use them, like generating personal documentation and lightweight versioning.",
        "primary": "UI-Information"
    },
    {
        "caption": "TeleGam",
        "img": "researchImages/Telegam.PNG",
        "bibEntry": "<div class=\"csl-entry\">Fred Hohman, Arjun Srinivasan, and Steven M. Drucker. 2019. TeleGam: Combining Visualization and Verbalization for Interpretable Machine Learning. In <i>Proceedings of the IEEE Visualization Conference</i>.IEEE, Vancouver, BC, Canada, 2019.</div>",
        "tags": {
            "collaborators": [
                "Hohman",
                "Srinivasan"
            ],
            "subject": [
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2019"
            ],
            "publication": [
                "VIS"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/telegam-vis19.pdf",
        "abstract": "While machine learning (ML) continues to nd success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difcult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benets of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specically, we present a prototype system,TELEGAM , that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describeTELEGAM 's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TELEGAM can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.",
        "primary": "Visualization"
    },
    {
        "caption": "AIfinnity",
        "img": "researchImages/AIfinnity.png",
        "bibEntry": "<div class=\"csl-entry\">Angel Alexander Caqbrera, Marco Tulio Ribeiro, Bongshin Lee, Robert A. DeLine, Adam Perer, Steven M. Drucker. 2019. What Did My AI Learn? How Data Scientists Make Sense of Model Behavior. In <i>Transactions on Computer Human Interaction</i>.ACM, 2022.</div>",
        "tags": {
            "collaborators": [
                "Cabrera",
                "Ribeiro",
                "Lee",
                "DeLine",
                "Perer"
            ],
            "subject": [
                "Information",
                "Visualization",
                "ML"
            ],
            "year": [
                "2022"
            ],
            "publication": [
                "TOCHI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/AIfinnity.pdf",
        "abstract": "Data scientists require rich mental models of how AI systems behave to effectively train, debug, and work with them. Despite the prevalence of AI analysis tools, there is no general theory describing how people make sense of what their models have learned. We frame this process as a form of sensemaking and derive a framework describing how data scientists develop mental models of AI behavior. To evaluate the framework, we show how existing AI analysis tools fit into this sensemaking process and use it to design AIFinnity, a system for analyzing image-and-text models. Lastly, we explored how data scientists use a tool developed with the framework through a think-aloud study with 10 data scientists tasked with using AIFinnity to pick an image captioning model. We found that AIFinnity’s sensemaking workflow reflected participants’ mental processes and enabled them to discover and validate diverse AI behaviors.",        
        "primary": "ML"
    },
    {
        "caption": "NL2VIS",
        "img": "",
        "bibEntry": "<div class=\"csl-entry\">Arjun Srinivasan, Nikhula Nyapathy, Bongshin Lee, Steven M. Drucker, John Stasko, 2021. Collecting and Characterising Natural Language Utterances for Specifying Data Visualizations. In <i>Proceedings of the ACM Conference on Human Computer Interaction</i>.ACM SIGCHI , 2021.</div>",
        "tags": {
            "collaborators": [
                "Srinivasan",
                "Nyapathy",
                "Lee",
                "Stasko"
            ],
            "subject": [
                "HCI",
                "Data",
                "Visualization"
            ],
            "year": [
                "2021"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "Visualization"
    },
    {
        "caption": "Data Visceralization",
        "img": "",
        "bibEntry": "<div class=\"csl-entry\">Benjamin Lee, Dave Brown, Bongshin Lee, Christophe Hurter, Steven M. Drucker, 2021. Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality, In <i>IEEE Transactions on Visualization and Computer Graphics</i>.IEEE VIS, 2021.</div>",
        "tags": {
            "collaborators": [
                "Lee",
                "Brown",
                "Lee",
                "Hurter",
                "Dwyer"
            ],
            "subject": [
                "Graphics",
                "HCI",
                "Data",
                "Visualization"
            ],
            "year": [
                "2021"
            ],
            "publication": [
                "Infovis"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "Visualization"
    },
    {
        "caption": "Fork It",
        "img": "",
        "bibEntry": "<div class=\"csl-entry\">Nathaniel Weinman, Steven M. Drucker, Titus Barik, Robert DeLine, 2021. Fork it: Supporting Stateful Alternatives in Computations Notebooks. In <i>Proceedings of the ACM Conference on Human Computer Interaction</i>.ACM SIGCHI , 2021.</div>",
        "tags": {
            "collaborators": [
                "Weinman",
                "Nyapathy",
                "Lee",
                "Stasko"
            ],
            "subject": [
                "HCI",
                "Data",
                "Visualization"
            ],
            "year": [
                "2021"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "Visualization"
    },
    {
        "caption": "AI Model Communication",
        "img": "researchImages/AIComm.png",
        "bibEntry": "<div class=\"csl-entry\">Almahmoud, Jumana, Robert DeLine, Steven M. Drucker, 2021. How Teams Communicate about the Quality of ML Models: A Case Study at an International Technology Company. In <i>Proceedings of the ACM Conference on Human Computer Interaction Group</i>.ACM Group , 2021.</div>",
        "tags": {
            "collaborators": [
                "Almahmoud",
                "DeLine"
            ],
            "subject": [
                "HCI",
                "Data"
            ],
            "year": [
                "2021"
            ],
            "publication": [
                "Group"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "UI-Information"
    },
    {
        "caption": "ReuseStrategies",
        "img": "researchImages/Reuse.png",
        "bibEntry": "<div class=\"csl-entry\">Will Eperson, April Wang, Robert DeLine, Steven M. Drucker, 2022. Strategies for Reusing and Sharing among Data Scientists in Software Teams. In <i>Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Practice</i>.ICSE-SEIP, 2022.</div>",
        "tags": {
            "collaborators": [
                "Epperson",
                "Yang",
                "DeLine"
            ],
            "subject": [
                "HCI",
                "Data"
            ],
            "year": [
                "2022"
            ],
            "publication": [
                "ICSE"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "UI-Information"
    },
    {
        "caption": "DITL",
        "img": "researchImages/DITL.png",
        "bibEntry": "<div class=\"csl-entry\">April Wang, Will Eperson, Robert DeLine, Steven M. Drucker, 2022. Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis. In <i>Proceedings of the ACM Conference on Human Factors in Computer Systems</i>.ACM CHI, 2022.</div>",
        "tags": {
            "collaborators": [
                "Yang",
                "Epperson",
                "DeLine"
            ],
            "subject": [
                "HCI",
                "Data"
            ],
            "year": [
                "2022"
            ],
            "publication": [
                "CHI"
            ]
        },
        "pdf": "",
        "abstract": "",
        "primary": "UI-Information"
    },
    {
        "caption": "Composites",
        "img": "researchImages/Composites.png",
        "bibEntry": "<div class=\"csl-entry\">Hariharan Subramonyam, Eytan Adar, Steven M. Drucker, 2022. Composites: A Tangible Interaction Paradigm for Visual Data Analysis in Design Practice. In <i>Advanced Visual Interfaces Conference</i>.ACM, 2022.</div>",
        "tags": {
            "collaborators": [
                "Subramonyam",
                "Adar"
            ],
            "subject": [
                "Information",
                "Visualization",
                "HCI"
            ],
            "year": [
                "2022"
            ],
            "publication": [
                "AVI"
            ]
        },
        "pdf": "https://docs.google.com/viewer?url=https://github.com/StevenMDrucker/ResearchContent/raw/master/papers/composites.pdf",
        "abstract": "Conventional tools for visual analytics emphasize a linear production workflow and lack organic “work surfaces.” A better surface would simultaneously support collaborative visualization construction, data and design exploration, and reasoning. To facilitate data driven design within existing design tools such as card sorting, we introduce Composites, a tangible, augmented reality interface for constructing visualizations on large surfaces. In response to the placement of physical sticky-notes, Composites projects visualizations and data onto large surfaces. Our spatial grammar allows the designer to flexibly construct visualizations through the use of the notes. Similar to affinity-diagramming, the designer can “connect” the physical notes to data, operations, and visualizations which can then be re-arranged based on creative needs. We develop mechanisms (sticky interactions, visual hinting, etc.) to provide guiding feedback to the end-user. By leveraging low-cost technology, Composites extends a working surface to support a broad range of workflows without limiting creative design thinking. While machine learning (ML) continues to nd success in solving previously-thought hard problems, interpreting and exploring ML models remains challenging. Recent work has shown that visualizations are a powerful tool to aid debugging, analyzing, and interpreting ML models. However, depending on the complexity of the model (e.g., number of features), interpreting these visualizations can be difcult and may require additional expertise. Alternatively, textual descriptions, or verbalizations, can be a simple, yet effective way to communicate or summarize key aspects about a model, such as the overall trend in a model's predictions or comparisons between pairs of data instances. With the potential benets of visualizations and verbalizations in mind, we explore how the two can be combined to aid ML interpretability. Specically, we present a prototype system,TELEGAM , that demonstrates how visualizations and verbalizations can collectively support interactive exploration of ML models, for example, generalized additive models (GAMs). We describeTELEGAM 's interface and underlying heuristics to generate the verbalizations. We conclude by discussing how TELEGAM can serve as a platform to conduct future studies for understanding user expectations and designing novel interfaces for interpretable ML.",
        "primary": "UI-Information"
    }
]
